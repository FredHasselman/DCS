[
["index.html", "A Complex Systems Approach to Study Human Nature 1 An introduction to the analytical toolbox of Complexity Science", " A Complex Systems Approach to Study Human Nature Fred Hasselman 12-05-2017 1 An introduction to the analytical toolbox of Complexity Science Figure 1.1: From Grip on Complexity This workshop will provide an introduction to some of the formal models, research methods and analytical techniques that allow for the study of human behaviour from a complex systems perspective. Complexity research transcends the boundaries between the classical scientific disciplines and is a hot topic in physics, mathematics, biology, economy and psychology. Its focus is a description and explanation of behaviour based on interaction dominant dynamics: Many processes interact on different temporal and spatial scales and behaviour emerges out of those interactions through physical processes such as self-organization or soft-assembly. Contrary to what the name might suggest, complexity research is often about finding simple models or collective variables with which a wide range of different behavioural modes can be described. This approach differs fundamentally from the more classical approaches in which behaviour is considered the additive result of many independent, component processes (component dominant dynamics) and the goal of research is to identify efficient causes of behaviour. The main focus of the workshop will be hands-on experience with data-analysis using the R statistical computing environment. No special background knowledge is required to participate. Tentative program I. Introduction to the mathematics of change * Modelling (nonlinear) growth and Deterministic Chaos * Predator-Prey dynamics * Basic timeseries analysis II. Quantifying Recurrences in State Space * Takens’ Theorem and State-Space reconstruction * Recurrence Quantification Analysis of continuous and categorical data * Cross-Recurrence Quantification Analysis of dyadic interaction III. Fractal Scaling, Network Topology and Early Warning Signals * Scaling phenomena in time and trial series of human behaviour and physiology * Small-world and Scale-free networks * Early Warning Signals in clinical interventions "],
["using-r.html", "Using R!", " Using R! I recommend installing the latest version of R and RStudio. Rstudio is not strictly necessary, but especially new users will have a somewhat more comfortable expeRience. If you are completeley new to R you might want to check these notes Packages needed for the assignments You’ll need to install the following packages, just copy and paste the command in R. Depending on your computer and internet connection, this might take a while to complete. If you run into any errors, skip the package and try the others, e.g. by adding them through the user interface of Rstudio (Tools &gt;&gt; Installl Packages...). install.packages(c(&quot;devtools&quot;, &quot;rio&quot;,&quot;plyr&quot;, &quot;tidyverse&quot;,&quot;Matrix&quot;, &quot;ggplot2&quot;, &quot;lattice&quot;, &quot;latticeExtra&quot;, &quot;grid&quot;, &quot;gridExtra&quot;, &quot;scales&quot;, &quot;dygraphs&quot;,&quot;rgl&quot;, &quot;plot3D&quot;, &quot;fractal&quot;, &quot;nonlinearTseries&quot;, &quot;crqa&quot;,&quot;signal&quot;, &quot;sapa&quot;, &quot;ifultools&quot;, &quot;pracma&quot;, &quot;nlme&quot;, &quot;lme4&quot;, &quot;lmerTest&quot;, &quot;minpack.lm&quot;, &quot;igraph&quot;,&quot;qgraph&quot;,&quot;graphicalVAR&quot;,&quot;bootGraph&quot;,&quot;IsingSampler&quot;,&quot;IsingFit&quot;), dependencies = TRUE) NOTE: Sometimes R will ask whether you want to install a newer version of a package which still has to be built from the source code. I would suggest to select NO, because this will take more time and might cause problems. Files on GitHub All the files (data, scripts and files that generated this document) are in a repository on Github. Github keeps track of all the different versions of the files in a repository. If you want to download a file that is basically a text file (e.g. an R script), find a button on the page named raw, press it and copy the text in your browser, or save it as a text file. For non-text files, a download button will be present somewhere on the page. First, download from Github and type source('nlRtsa_SOURCE.R'), or, source it directly from Github if you have package devtools installed. library(devtools) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) 1.0.1 Timeseries in R There are many different ways to handle and plot timeseries data in R, I summarised some of them in the notes This document This text was transformed to HTML, PDF en ePUB using bookdown(Xie 2016a) in RStudio, the graphical user interface of the statistical language R (R Core Team 2016). bookdown makes use of the R version of markdown called Rmarkdown (Allaire et al. 2016), together with knitr (Xie 2016c) and pandoc. We’ll use some web applications made in Shiny (Chang et al. 2017) Other R packages used are: DT (Xie 2016b), htmlTable (Gordon 2017), plyr (Wickham 2016), dplyr (Wickham and Francois 2016),tidyr (Wickham 2017), png (Urbanek 2013), rio (Chan and Leeper 2016). --> Bibliography "],
["assignments-how-to.html", "Assignments: How to … ", " Assignments: How to … These assignments were designed to prepare you for “real world” modelling and data analysis problems. That is, after completing the assignments you should be able to decide whether the phenomenon you study could benefit from a complex systems approach and which type of analyses would be a good place to start. The models and techniques discussed here are not a definite collection of available techniques, this is really just the tip of the iceberg. General Guidelines Read the instructions carefully. Do not skip any of the steps. Do not copy-paste from the assignment text into a spreadsheet or syntax editor (except for text in code blocks). Study the solutions and lecture notes. "],
["moc1ass.html", "2 Modelling (nonlinear) growth", " 2 Modelling (nonlinear) growth In this assignment you will build two (relatively) simple one-dimensional maps in R, based on an example in a spreadsheet. Go to the followin GoogleSheet and save a copy on your computer, you can use your favourite spreadsheet software (e.g., Excel, Numbers), or, copy it to a new GoogleSheet if you prefer Google. We will start with the Linear Map and then proceed to the slightly more complicated Logistic Map (aka Quadratic map). Be sure to check the solutions of the assigment and the examples of different ways to visualize the time series in R "],
["the-linear-map-in-a-spreadsheet.html", "2.1 The Linear Map in a Spreadsheet", " 2.1 The Linear Map in a Spreadsheet Equation (2.1) is the ordinary difference equation (ODE) discussed on the slides and is called the Linear Map: \\[\\begin{equation} Y_{t+1} = Y_{t=0} + r*Y_t \\tag{2.1} \\end{equation}\\] In these excersises you will simulate time series produced by the change process in equation @(eq:linmap) for different parameter settings of the growth-rate parameter \\(r\\) (the control parameter) and the initial conditions \\(Y_0\\). Simulation is obvioulsy different from a statistical analysis in which parameters are estimated from a data set. The goal of these assignments is to get a feeling for what a dynamical model is, and how it is different from a linear statistical regression model like GLM. Before you begin, be sure to check the following settings: Open a Microsoft Excel worksheet, a Google sheet, or other spreadsheet. Check whether the spreadsheet uses a ‘decimal-comma’ (\\(0,05\\)) or ‘decimal-point’ (\\(0.05\\)) notation. The numbers given in the assignments of this course all use a ‘decimal-point’ notation. Check if the $ symbol fixes rows and columns when it used in a formula in your preferred spreadsheet program. This is the default setting in Microsoft Excel, Numbers and Google. If you use one of those programs you are all set, otherwise you will have to replace the $ used in the assignments with the one used by your software. Open the spreadsheet and look at the Linear map tab. The r in cell A5 is the control parameter. It currently has the value \\(1.08\\) in cellB5. The cell labelled \\(Y_0\\) in A6 is the initial value at \\(t=0\\). It has the value \\(0.1\\) in cell B6. The A column also contains \\(Y_t\\), the output of the linear map. Click on A10, in the fomula bar you can see this refers to the initial value B6. Click on A11, in the fomula bar you can see the very simple implementation of the linear map. The value of cell A11 (i.e. \\(Y_{t=1}\\)) will be calculated by multiplying the value of cell B5 (parameter r) with the value of cell A10 (the previous value, here: \\(Y_{t=0}\\)). Remember what it is we are doing! We are calculating \\(Y_{t=2}\\) (i.e. ‘behaviour’ in the future) and this is completely determined by \\(Y_{t=1}\\) (i.e., the previous step) and the parameter r. The control parameter If you change the values in cells B5 and B6 you will see an immediate change in the graph. To study model behaviour, try the following growth parameters: \\(r = -1.08\\) \\(r = 1,08\\) \\(r = 1\\) \\(r = -1\\) Dependence on initial conditions? Change the initial value \\(Y_0\\) in cell B6 to \\(10\\). Subsequently give the growth parameter in cell B5 the following values: \\(0.95\\) \\(-0.95\\). "],
["the-logistic-map-in-a-spreadsheet.html", "2.2 The Logistic Map in a spreadsheet", " 2.2 The Logistic Map in a spreadsheet The Logistic Map takes the following functional form: \\[\\begin{equation} Y_{t+1} = r*Y_t*(1-Y_t) \\tag{2.2} \\end{equation}\\] Open the spreadsheet and look at the Logistic map tab. The setup is the same as for the linear map, except of course the change process that is implemented. The A column also contains \\(Y_t\\), the output of the Logistic map, currently the behaviour looks a lot like an S-curve, a logistic function. Click on A10, in the fomula bar you can see this refers to the initial value B6. Click on A11, in the fomula bar you can see the very simple implementation of the Logistic map. Again, the value of cell A11 (\\(Y_{t=1}\\)) is calculated by multiplying the value of cell B5 (parameter r) with the value of cell A10 (the current value \\(Y_t\\), in this case \\(Y_{t=0}\\)). The only difference is we are also multiplying by \\((1-Y_t)\\). We established that r controls growth, what could be the function of \\((1-Y_t)\\)? The control parameter To study the behavior of the Logistic Map you can start playing around with the values in B5. Try the following settings for \\(r\\): \\(r = 0.9\\) \\(r = 1.9\\) \\(r = 2.9\\) \\(r = 3.3\\) \\(r = 3.52\\) \\(r = 3.9\\) The return plot Set \\(r\\) at \\(4.0\\): Now copy A10:A310 to B9:B309 (i.e., move it one cell to the right, and one cell up) Select both columns (A10 to B309!) and make a scatter-plot The plot you just produced is a so called return plot, in which you have plotted \\(Y_{t+1}\\) against \\(Y_t\\). Can you explain the pattern you see (a ‘parabola’) by looking closely at the functional form of the Logistic Map? (hint: it’s also called Quadratic Map) What do you expect the return plot of the Linear Map will look like? Go back and try it! 2.2.1 Sensitive dependence on initial conditions Go to the following GoogleSheet and download or copy it. This sheet displays the extreme sensitivity on initial conditions of systems displaying chaotic behaviour, more commonly known as: The Butterfly Effect Imagine these are observed timseries from two subjects in a response time experiment. These subjects are perfect ‘twins’: The formula describing their behaviour is exactly the same (it’s the Quadratic Map, check it!) The control parameter, controlling the behavioural mode is exactly the same (\\(r=4\\)). The only difference is that they have a tiny discrepancy in intitial conditions in cell D6 of \\(0.01\\). Not tiny enough? Change it to become even smaller: \\(0.001\\) \\(0.0001\\) \\(0.00001\\) etc. What happens as you make the discrepency smaller and smaller? "],
["moc1R.html", "2.3 Using R or Matlab to simulate these maps.", " 2.3 Using R or Matlab to simulate these maps. The best (and easiest) way to simulate these simple models is to create a function which takes as input the parameters (\\(Y_0\\), \\(r\\)) and a variable indicating the length of the time series. For example for the Linear Map: # A function with three arguments linearMap &lt;- function(Y0 = 0, r = 1, N = 100){ # Initialize Y as vector of Y0, followed by N-1 empty slots (NA). Y &lt;- c(Y0, rep(NA,N-1)) # Then, you need create the iteratation for(i in 1:(N-1)){ Y[i+1] &lt;- # !!Implement the linear map here!! } return(Y) } Copy the code and implement the linear map. When you are done, you need to initialize the function, select the code and run it. The environment will now contain a function called linearMap Generate some data by calling the function using \\(Y0=0.1\\), \\(r=1.08\\) and \\(N=100\\) and store the result in a variable. 2.3.1 Plot the timeseries Creating the time series graphs and the return plot should be easy if the function linearMap returns the time series. R and Matlab have specialized objects to represent timeseries, and functions and packages for timeseries analysis. They are especially convenient for plotting time and date information on the X-axis. See the notes on the time series object for more information. -->"],
["moc2ass.html", "3 EXTRA", " 3 EXTRA In this assignment you’ll build a more sophisticated growth model and look at its properties. The model will be the growth model by Van Geert (1991). You can try to code the models following the hints in section 2.3. | jump straight to the solutions | "],
["the-growth-model-by-van-geert-1991.html", "The growth model by Van Geert (1991)", " The growth model by Van Geert (1991) The growth model by Van Geert has the following form: \\[\\begin{equation} L_{t+1} = L_t * (1 + r - r * \\frac{L_t}{K}) \\tag{3.1} \\end{equation}\\] Note the similarities to Equation (2.2), the (stylized) logistic map. 3.0.1 The growth model in a spreadsheet Before you begin, be sure to check the following settings (same as first asignment): Open a Microsoft Excel worksheet or a Google sheet Check whether the spreadsheet uses a ‘decimal-comma’ (\\(0,05\\)) or ‘decimal-point’ (\\(0.05\\)) notation. The numbers given in the assignments of this course all use a ‘decimal-point’ notation. Check if the $ symbol fixes rows and columns when it used in a formula in your preferred spreadsheet program. This is the default setting in Microsoft Excel and Google Sheets. If you use one of those programs you are all set. To build it take the sheet in assignment 2 as a template. Define the appropriate constants (\\(r\\) in A5, \\(L_0\\) in A6) and prepare the necessary things you need for building an iterative process. In particular, add the other parameter that appears in Van Geert’s model: Type \\(K\\) in cell A7. This is the carrying capacity. It receives the value \\(1\\) in cell B7. Start with the following values: \\(r = 1.2\\) \\(L_0 = 0.01\\) Take good notice of what is constant (parameters \\(r\\) and \\(K\\)), for which the $ must be used, and what must change on every iterative step (variable \\(L_t\\)). Take about \\(100\\) steps. Create the graphs You can start playing with the values for the parameters and the initial values in cells B5, B6 and B7. To study its behavior, be sure to try the following growth parameters: \\(r = 1.2\\) \\(r = 2.2\\) \\(r = 2.5\\) \\(r = 2.7\\) \\(r = 2.9\\) For the carrying capacity \\(K\\) (cell B7) you can try the following values: \\(K = 1.5\\) \\(K = 0.5\\). (Leave \\(r = 2.9\\). Mind the value on the vertical axis!) Changes in the values of \\(K\\) have an effect on the height of the graph. The pattern itself also changes a bit. Can you explain why this is so? "],
["conditional-growth-jumps-and-stages.html", "3.1 Conditional growth: Jumps and Stages", " 3.1 Conditional growth: Jumps and Stages Auto-conditional jumps Suppose we want to model that the growth rate \\(r\\) increases after a certain amount has been learned. In general, this is a very common phenomenon, for instance: when becoming proficient at a skill, growth (in proficiency) is at first slow, but then all of a sudden there can be a jump to the appropriate (and sustained) level of proficiency. Take the model you just built as a starting point with \\(r = 0.1\\) (B5) Type \\(0.5\\) in C5. This will be the new parameter value for \\(r\\). Build your new model in column B (leave the original in A). Suppose we want our parameter to change when a growth level of \\(0.2\\) is reached. We???ll need an IF statement which looks something like this: IF \\(L &gt; 0.2\\) then use the parameter value in C5, otherwise use the parameter value in B5. Excel has a built in IF function (may be ALS in Dutch). In the first cell in which calculations should start, press \\(=\\) and then from the formula list choose the IF function, or just type it. Try to figure out what you have to do. In the Logical_test box you should state something which expresses \\(L &gt; 0.2\\). The other fields tell Excel what to do when this statement is TRUE (then use parameter value in C5) or when it is FALSE (then use paramter value in B5). Note: the word value might be misleading; you can also input new statements. Make a graph in which the old and the new conditional models are represented by lines. Try changing the value of \\(r\\) in C5 into: \\(1, 2, 2.8, 2.9, 3\\). Auto-conditional stages Another conditional change we might want to explore is that when a certain growth level is reached the carrying capacity K increases, reflecting that new resources have become available to support further growth. Now we want \\(K\\) to change, so type \\(1\\) in B7, \\(2\\) in C7. Build your model in column C. Follow the same steps as above, but now make sure that when \\(L &gt; 0.99\\), \\(K\\) changes to the value in C7. Keep \\(r = 0.2\\) (B5). If all goes well you should see two stages when you create a graph of the timeseries in column C. Change \\(K\\) in C7 to other values. Try to also change the growth rate r after reaching \\(L &gt; 0.99\\) by referring to C5. Start with a value of \\(0.3\\) in C5. Set \\(K\\) in C7 to \\(2\\) again. Also try \\(1, 2.1, 2.5, 2.6, 3\\). Connected growers You can now easly model coupled growth processes, in which the values in one series serve as the trigger for for parameter changes in the other process. Try to recreate the Figure of the connected growers printed in the chapter by Van Geert. 3.1.0.1 Demonstrations of dynamic modeling using spreadsheets See the website by Paul Van Geert, scroll down to see models of: Learning and Teaching Behaviour Modification Connected Growers Interaction during Play -->"],
["predator-prey-dynamics.html", "4 Predator-Prey dynamics", " 4 Predator-Prey dynamics In this assignment we will look at a 2D coupled dynamical system: the Predator-Prey model (aka Lotka-Volterra equations). "],
["predator-prey-model.html", "4.1 Predator-prey model", " 4.1 Predator-prey model The dynamical system is given by the following set of first-order differential equations, one represents changes in a population of predators, (e.g., Foxes: \\(f_F(R_t,F_t)\\) ), the other represents changes in a population of prey, (e.g., Rabbits: \\(f_R(R_t,F_t)\\) ). \\[\\begin{align} \\frac{dR}{dt}&amp;=(a-b*F)*R \\\\ \\\\ \\frac{dF}{dt}&amp;=(c*R-d)*F \\tag{4.1} \\end{align}\\] This is not a difference equation but a differential equation, which means building this system is not as straightforward as was the case in the previous assignments. Simulation requires a numerical method to ‘solve’ this differential equation for time, which means we need a method to approach, or estimate continuous time in discrete time. Below you will receive a speed course in one of the simplest numerical procedures for integrating differential equations, the Euler method. 4.1.1 Euler Method A general differential equation is given by: \\[\\begin{equation} \\frac{dx}{dt} = f(x) \\tag{4.2} \\end{equation}\\] Read it as saying: “a change in \\(x\\) over a change in time is a function of \\(x\\) itself”. This can be approximated by considering the change to be over some constant, small time step \\(\\Delta\\): \\[\\begin{equation} \\frac{(x_{t+1} = x_t)}{\\Delta} = f(x_t) \\tag{4.3} \\end{equation}\\] After rearranging the terms a more familiar form reveals itself: \\[\\begin{align} x_{t+1} &amp;= x_t &amp;= f(x_t) * \\Delta \\\\ x_{t+1} &amp;= f(x_t) * \\Delta + x_t \\tag{4.4} \\end{align}\\] This looks like an ordinary iterative process, \\(\\Delta\\) the time constant determines the size of time step taken at every successive iteration. For a 2D system with variables R and F on would write: \\[\\begin{align} R_{t+1} &amp;= f_R(R_t,Ft) * \\Delta + R_t \\\\ F_{t+1} &amp;= f_F(R_t,F_t) * \\Delta + F_t \\tag{4.5} \\end{align}\\] 4.1.2 Coupled System in a spreadsheet Implement the model in a spreadsheet by substituting \\(f_R(R_t,Ft)\\) and \\(f_F(R_t,F_t)\\) by the differential equations for Foxes and Rabbits given above. Start with \\(a = d = 1\\) and \\(b = c = 2\\) and the initial conditions \\(R_0 = 0.1\\) and \\(F_0 = 0.1\\). Use a time constant of \\(0.01\\) and make at least \\(1000\\) iterations. Visualize the dynamics of the system by plotting: \\(F\\) against \\(R\\) (i.e., the state space) \\(R\\) and \\(F\\) against time (i.e., the timeseries) in one plot. Starting from the initial predator and prey population represented by the point \\((R, F) = (0.1, 0.1)\\), how do the populations evolve over time? Try to get a grip on the role of the time constant by increasing and decreasing it slightly (e.g. \\(\\Delta = 0.015\\)) for fixed parameter values. (You might have to add some more iterations to complete the plot). What happens to the plot? Hint: Remember that \\(\\Delta\\) is not a fundamental part of the dynamics, but that it is only introduced by the numerical integration (i.e., the approximation) of the differential equation. It should not change the dynamics of the system, but it has an effect nonetheless. | jump to solution | 4.1.3 Tip of the iceberg There is much more to tell about dynamic modelling, see the notes for some more advanced topics. -->"],
["basic-timeseries-analysis.html", "5 Basic Timeseries Analysis", " 5 Basic Timeseries Analysis Most of the basic timeseries analyses can be performed in SPSS, because many of you will be familiar with the software we present the first assignments mainly as SPSS instructions, but you can go ahead an try your preferred environment for (statistical) computing. "],
["time-series-analysis-in-spss-17-and-higher.html", "5.1 Time series analysis in SPSS (17 and higher)", " 5.1 Time series analysis in SPSS (17 and higher) 5.1.1 Nonlinear Growth curves in SPSS Open the file Growthregression.sav, it contains two variables: Time and Y(t). This is data from an iteration of the logistic growth differential equation you are familiar with by now, but let’s pretend it’s data from one subject measured on 100 occasions. Plot Y(t) against Time Recognize the shape? To get the growth parameter we’ll try to fit the solution of the logistic flow with SPSS nonlinear regression Select nonlinear… from the Analysis &gt;&gt; Regression menu. Here we can build the solution equation. We need three parameters: a. Yzero, the initial condition. b. K, the carrying capacity. c. r, the growth rate. Fill these in where it says parameters give all parameters a starting value of \\(0.01\\) Take a good look at the analytic solution of the (stilized) logistic flow: \\[ Y(t) = \\frac{K * Y_0}{Y_0 + \\left(K-Y_{0}\\right) * e^{(-K*r*t)} } \\] Tr to build this equation, the function fo \\(e\\) is called EXP in SPSS (Function Group &gt;&gt; Arithmetic) Group terms by using parentheses as shown in the equation. If you think you have built the model correctly, click on Save choose predicted values. Then paste your syntax and run it! Check the estimated parameter values. Check \\(R^2\\)!!! Plot a line graph of both the original data and the predicted values. (Smile) A polynomial fishing expedition: Create time-varying covariates of \\(Y(t)\\): COMPUTE T1=Yt * Time. COMPUTE T2=Yt * (Time ** 2). COMPUTE T3=Yt * (Time ** 3). COMPUTE T4=Yt * (Time ** 4). EXECUTE. Use these variables as predictors of \\(Y(t)\\) in a regular linear regression analysis. This is called a polynomial regression: Fitting combinations of curves of different shapes on the data. Before you run the analysis: Click Save Choose Predicted Values: Unstandardized Look at \\(R^2\\). This is also almost 1. Which model is better? Think about this: Based o the results o the linear regression what can yo tell about the growth rate, the carrying capacity or the initial condition? Create a line graph of \\(Y(t)\\), plot the predicted values of the nonlinear regression and the unstandardized predicted values of the linear polynomial regression against time in one figure. Now you can see that the shape is approximated by the polynomials, but it is not quite the same. Is this really a model of a growth process as we could encounter it in nature? | jump to solution | 5.1.2 Correlation functions and AR-MA models Download the file series.sav from blackboard. It contains three time series TS_1, TS_2 and TS_3. As a first step look at the mean and the standard deviation (Analyze &gt;&gt; Descriptives). Suppose these were time series from three subjects in an experiment, what would you conclude based on the means and SD’s? Let’s visualize these data. Go to Forecasting &gt;&gt; Time Series &gt;&gt; Sequence Charts. Check the box One chart per variable and move all the variables to Variables. Are they really the same? Let’s look at the ACF and PCF Go to Analyze &gt;&gt; Forecasting &gt;&gt; Autocorrelations. Enter all the variables and make sure both Autocorrelations (ACF) and Partial autocorrelations (PACF) boxes are checked. Click Options, and change the Maximum Number of Lags to 30. Use the table to characterize the time series: SHAPE INDICATED MODEL Exponential, decaying to zero Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model Alternating positive and negative, decaying to zero Autoregressive model. Use the partial autocorrelation plot to help identify the order. One or more spikes, rest are essentially zero Moving average model, order identified by where plot becomes zero. Decay, starting after a few lags Mixed autoregressive and moving average model. All zero or close to zero Data is essentially random. High values at fixed intervals Include seasonal autoregressive term. No decay to zero Series is not stationary. You should have identified just one time series with autocorrelations: TS_2. Try to fit an ARIMA(p,0,q) model on this time series. Go to Analyze &gt;&gt; Forecasting &gt;&gt; Create Model, and at Method (Expert modeler) choose ARIMA. Look back at the PACF to identify which order (p) you need (last lag value at which the correlation is still significant). This lag value should go in the Autocorrelation p box. Start with a Moving Average q of one. The time series variable TS_2 is the Dependent. You can check the statistical significance of the parameters in the output under Statistics, by checking the box Parameter Estimates. This value for p is probably too high, because not all AR parameters are significant. Run ARIMA again and decrease the number of AR parameters by leaving out the non-significant ones. By default SPSS saves the predicted values and 95% confidence limits (check the data file). We can now check how well the prediction is: Go to Graphs &gt;&gt; Legacy Dialogs &gt;&gt; Line. Select Multiple and Summaries of Separate Variables. Now enter TS_2, Fit_X, LCL_X and UCL_X in Lines Represent. X should be the number of the last (best) model you fitted, probably 2. Enter TIME as the Category Axis. In the simulation part of this course we have learned a very simple way to explore the dynamics of a system: The return plot. The time series is plotted against itself shifted by 1 step in time. Create return plots (use a Scatterplot) for the three time series. Tip: You can easily create a t+1 version of the time series by using the LAG function in a COMPUTE statement. For instance: COMPUTE TS_1_lag1 = LAG(TS_1) Are your conclusions about the time series the same as in 3. after interpreting these return plots? | jump to solution | "],
["notes-on-tsa-in-r.html", "5.2 Notes on TSA in R", " 5.2 Notes on TSA in R If you use R the command below will install all the packages we will use during the entire course on you private computer. This might take too long on a university PC, just install the packages you need for an assignment each session. 5.2.1 Importing data in R If you have package rio installed in R, you can load the data directly into the local environment. Follow the link, e.g. for series.sav. On the Github page, find a button marked Download (or Raw for textfiles). Copy the url associated with the Download button on Github (right-clik). The copied path should contain the word ‘raw’ somewhere in the url. Call import(url): series &lt;- import(&quot;https://github.com/FredHasselman/DCS/raw/master/assignmentData/BasicTSA_arma/series.sav&quot;) You can use the function arima(), acf() and pacf() in R (Matlab has functions that go by slightly different names, check the Matlab Help pages). There are many extensions to these linear models, check the CRAN Task View on Time Series Analysis to learn more (e.g. about package zoo and forecast). | jump to solution | -->"],
["part-ii-quantifying-recurrences-in-state-space.html", "6 (PART) II. Quantifying Recurrences in State Space", " 6 (PART) II. Quantifying Recurrences in State Space "],
["phase-space-reconstruction-and-rqa.html", "7 Phase Space Reconstruction and RQA", " 7 Phase Space Reconstruction and RQA You can use R to run RQA analyses. These assignments assume you’ll use R, but you can also find a Matlab toolbox for RQA here: CRP toolbox You’ll need: library(rio) library(crqa) library(fractal) library(devtools) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) R-packages for Phase Space Reconstruction We’ll use package fractal and rgl to reconstruct some phase spaces. If you have sourced the functions in nlRtsa_SOURCE.R you can install and load these packages by running: in.IT(c(&quot;fractal&quot;,&quot;rgl&quot;)). The function in.IT() will first check if the requested packages are installed on the machine and only install them if they are not present. "],
["RQA.html", "7.1 Reconstruct the Lorenz attractor", " 7.1 Reconstruct the Lorenz attractor Package fractal includes the 3 dimensions of the Lorenz system in the chaotic regime. Run this code rgl::plot3d(lorenz) with both packages loaded to get an interactive 3D plot1 of this strange attractor. We’ll reconstruct the attractor based on just dimension X of the system using functions from package fractal, be sure to look at the manual pages of these functions. Package fractal and package nonlinearTseries use functions with similar names, do not load them together. Use lx &lt;- lorenz[1:2048,1] to reconstruct the phase space based on lx. Find an optimal embedding lag using timeLag(), use method = &quot;mutual&quot;. Find the embedding dimension, using FNN() Embed the timeseries using embedSeries(). Plot the reconstructed phase space. (You’ll need to use as.matrix() on the object created by embedSeries()) Use rgl::plot3d() to plot the reconstructed space. You’ll need the X Window System for interactive 3D plotting. This Linux desktop system comes installed in some way or form on most Mac and Windows systems.You can test if it is present by running rgl::open3d(), which will try to open an interactive plotting device↩ "],
["reconstruct-the-predator-prey-model.html", "7.2 Reconstruct the Predator-Prey model", " 7.2 Reconstruct the Predator-Prey model Use the same procedure as above to reconstruct the state space of the predator-prey system. (Look at the solutions to get a Foxes or Rabbit series). You should get a 2D state space, so 3D plotting might be a bit too much for this system. "],
["auto-recurrence-quantification-analysis.html", "7.3 (auto-) Recurrence Quantification Analysis", " 7.3 (auto-) Recurrence Quantification Analysis There are several packages which can perform (C)RQA analysis, we’ll use crqa because it can perform both continuous and categorical analyses. If you only have continuous data, you migh be better off using package nonlinearTseries, in this course we will only use package crqa. Package crqa() was designed to run categorical Cross-Recurrence Quantification Analysis (see Coco &amp; Dale (2014) and for R code see appendices in Coco &amp; Dale (2013)). We can trick it to run auto-RQA by providing the same timeseries for ts1 and ts2 and setting the parameter side to either &quot;upper&quot; or &quot;lower&quot; Perform an RQA on the reconstructed state space of the Lorenz system. You’ll need a radius (also called: threshold) in order to decide which points are close together (recurrent). crqa provides a function which will automatically select the best parameter settings: optimizeParam() Best way to ensure you are using the same parameters in each function is to create some lists with parameter settings (check the crqa manual to figure out what these parameters mean): # General settings for `crqa()` par0 &lt;- list(rescale = 1, normalize = 0, mindiagline = 2, minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, side = &quot;lower&quot;, checkl = list(do = FALSE, thrshd = 3, datatype = &quot;categorical&quot;,pad = TRUE) ) # Settings for `optimizeParam()` par &lt;- list(lgM = 20, steps = seq(1, 6, 1), radiusspan = 100, radiussample = 40, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, fnnpercent = 10, typeami = &quot;mindip&quot;) Get the optimal parameters using a radius which will give us 2%-5% recurrent points. ans &lt;- optimizeParam(ts1 = lx, ts2 = lx, par = par, min.rec = 2, max.rec = 5) Run the RQA analysis using the same settings with which the parameters were found. crqaOutput &lt;- crqa(ts1 = lx, ts2 = lx, delay = ans$delay, embed = ans$emddim, radius = ans$radius, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, side = par0$side, checkl = par0$checkl ) The output of crqa is a list with recurrence measures, the last entry is the recurrence plot. It is represented as a so-called sparse-matrix. This representation severely decreases the amount of memory occupied by the recurrence matrix. It is basically a list of indices of cells that contain a \\(1\\). The \\(0\\) do not need to be stored. In order to plot this matrix you could use image(), but this does not produce the recurrence plot as they are usually displayed, the y-axis needs to be flipped. We created a function which will take as input the list output of crqa, which wil be used to plot the recurrence matrix. If you have sourced the nlRtsa functions you can call plotRP.crqa(crqaOutput). | Jump to solutions | "],
["rqa-of-circle-tracing-data.html", "7.4 RQA of circle-tracing data", " 7.4 RQA of circle-tracing data Analyse the circle tracing data we recorded during the lecture. Study what happens to the RQA measures if you shuffle the temporal order (see e.g. the solution to th HRV assignments). Package fractal contains a function surrogate. This will create so-called constrained realisations of the time series. Look at the help pages of the function, or study the Surrogates Manual of the TISEAN software and create two surrogate series, one based on phase and one on aaft. Look at the RQA measures and think about which \\(H_0\\) should probably be rejected. If you want to be more certain, you’ll have to create a test (more surrogates). The TISEAN manual provides all the info you need to construct such a test: “For a minimal significance requirement of 95% , we thus need at least 19 or 39 surrogate time series for one- and two-sided tests, respectively. The conditions for rank based tests with more samples can be easily worked out. Using more surrogates can increase the discrimination power.” | Jump to solutions | -->"],
["categorical-and-cross-rqa-crqa.html", "8 Categorical and Cross-RQA (CRQA)", " 8 Categorical and Cross-RQA (CRQA) You can use R or Matlab to run RQA analyses. These assignments assume you’ll use R. You can find a Matlab toolbox for RQA here: CRP toolbox For R you’ll probably need: library(rio) library(crqa) library(fractal) library(devtools) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) "],
["CRQA.html", "8.1 Assignment: CRQA and Diagonal Profile", " 8.1 Assignment: CRQA and Diagonal Profile Create two variables for CRQA analysis, or use the \\(x\\) and \\(y\\) coordinates we roecorded during the lecture: y1 &lt;- sin(1:900*2*pi/67) y2 &lt;- sin(.01*(1:900*2*pi/67)^2) # Here are the circle trace data xy &lt;- import(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/RQA_circletrace/mouse_circle_xy.csv&quot;) y1 &lt;- xy$x y2 &lt;- xy$y You have just created two sine(-like) waves. We’ll examine if and how they are coupled in a shared phase space. As a first step plot them. Find an embedding delay (using mutual information) and an embedding dimension (if you calculate an embedding dimension using package fractal for each signal seperately, as a rule of thumb use the highest embedding dimension you find in further analyses). # General settings for `crqa()` par0 &lt;- list(rescale = 0, normalize = 0, mindiagline = 2, minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, side = &quot;both&quot;, checkl = list(do = FALSE, thrshd = 3, datatype = &quot;categorical&quot;,pad = TRUE) ) # Settings for `optimizeParam()` par &lt;- list(lgM = 20, steps = seq(1, 6, 1), radiusspan = 100, radiussample = 40, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, fnnpercent = 10, typeami = &quot;mindip&quot;) We can now create a cross recurrence matrix. Fill in the values you decided on. You can choose a radius automatically, look in the crqa manual. Get the optimal parameters using a radius which will give us 2%-5% recurrent points. Note: There is no rescaling of data, the sines were created in the same range. You can plot a matrix using image(). You could also check package nonlinearTseries. If you sourced the nlRtsa library, use plotRP.crqa() (ans &lt;- optimizeParam(ts1 = y1, ts2 = y2, par = par, min.rec = 2, max.rec = 5)) Run the CRQA and produce a plot of the recurrence matrix. Can you understand what is going on? For the simulated data: Explain the the lack of recurrent points at the beginning of the time series. For the circle trace: How could one see these are not determisnistic sine waves? Examine the synchronisation under the diagonal LOS. Look in the manual of crqa or Coco &amp; Dale (2014). To get the diagonal profile from the recurrence plot, use spdiags(). Make a plot of the diagonal profile. How far is the peak in RR removed from 0 (Line of Synchronisation)? Perform the same steps with a shuffled version (or use surrogate analysis!) of the data of timeseries \\(y1\\). You can use the embedding parameters you found earlier. NOTE: If you generate surrogate timeseries, make sure the RR is the same for all surrogates. Try to keep the RR in the same range by using the min.rec and max.rec settings of optimizeParam for each surrogate. | Jump to solutions | "],
["catCRQA.html", "8.2 Categorical CRQA", " 8.2 Categorical CRQA Package CRQA contains two categorical trial series from the “Friends”study. Lookup RDts1 and RDts2 in the help file. Also lookup the articles discussing the study to get an idea about these series here and here) In the paper accompanying the package by Coco &amp; Dale (2014) demonstrate the analysis of these series. Recreate this analysis, e.g. Figure 10 in the article including the CRQA measures. | Jump to solutions | -->"],
["cusp.html", "9 Potential and Catastrophe Models", " 9 Potential and Catastrophe Models The nonlinear regression assignment is written for SPSS, but you can try to use ‘R’ as well. Be sure to look at the solutions of the Basic Time Series Analysis) assignments. Using the cusp package "],
["fitting-the-cusp-catastrophe-in-spss.html", "9.1 Fitting the cusp catastrophe in SPSS", " 9.1 Fitting the cusp catastrophe in SPSS In the file Cusp Attitude.sav, you can find data from a (simulated) experiment. Assume the experiment tried to measure the effects of explicit predisposition and affective conditioning on attitudes towards Complexity Science measured in a sample of psychology students using a specially designed Implicit Attitude Test (IAT). Look at a Histogram of the difference score (post-pre) \\(dZY\\). This should look quite normal (pun intended). Perform a regular linear regression analysis predicting $dZY$ (Change in Attitude) from $\\alpha$ (Predisposition). Are you happy with the \\(R^2\\)? Look for Catastrophe Flags: Bimodality. Examine what the data look like if we split them across the conditions. Use \\(\\beta\\) (Conditioning) as a Split File Variable (Data &gt;&gt; Split File). And again, make a histogram of \\(dZY\\). Try to describe what you see in terms of the experiment. Turn Split File off. Make a Scatterplot of \\(dZY\\) (x-axis) and \\(\\alpha\\) (y-axis). Here you see the bimodality flag in a more dramatic fashion. Can you see another flag? Perhaps we should look at a cusp Catastrophe model: Go to Analyse &gt;&gt; Regression &gt;&gt; Nonlinear (also see Basic Time Series Analysis). First we need to tell SPSS which parameters we want to use, press Parameters. Now you can fill in the following: Intercept (Starting value \\(0.01\\)) B1 through B4 (Starting value \\(0.01\\)) Press Continue and use \\(dZY\\) as the dependent. Now we build the model in Model Expression, it should say this: Intercept + B1 * Beta * ZY1 + B2 * Alpha + B3 * ZY1 ** 2 + B4 * ZY1 ** 3 Run! And have a look at \\(R^2\\). The model can also be fitted with linear regression in SPSS, but we need to make some extra (nonlinear) variables using COMPUTE: BetaZY1 = Beta*ZY1 *(Bifurcation, splitting parameter). ZY1_2 = ZY1 ** 2 *(ZY1 Squared). ZY1_3 = ZY1 ** 3 *(ZY1 Cubed). Create a linear regression model with \\(dZY\\) as dependent and \\(Alpha\\), \\(BetaZY1\\) and \\(ZY1_2\\) en \\(ZY1_3\\) as predictors. Run! The parameter estimates should be the same. Finally try to can make a 3D-scatterplot with a smoother surface to have look at the response surface. HINT: this is a lot easier in R or Matlab perhaps you can export your SPSS solution. How to evaluate a fit? Check the last slides of lecture 8 in which the technique is summarised. The cusp has to outperform the pre-post model. "],
["the-cusp-package-in-r.html", "9.2 The cusp package in R", " 9.2 The cusp package in R Use this tutorial paper to fit the cusp in ‘R’ according to a maximum likelihood criterion. Grasman, R. P. P. P., Van der Maas, H. L. J., &amp; Wagenmakers, E. (2007). Fitting the Cusp Catastrophe in R: A cusp-Package. Start R and install package cusp Work through the Example I (attitude data) in the paper (Section 4: Examples, p. 12). Example II in the same section is also interesting, but is based on simulated data. Try to think of an application to psychological / behavioural science. -->"],
["nets.html", "10 Complex Networks", " 10 Complex Networks To complete these assignments you need: library(igraph) library(qgraph) library(devtools) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) "],
["basic-graphs.html", "10.1 Basic graphs", " 10.1 Basic graphs Create a small ring graph in igraph and plot it. g &lt;- graph.ring(20) - Look in the manual of `igraph` for a function that will get you the degree of each node. - ALso, get the average path length. Create a “Small world” graph and plot it. Get the degree, average path length and transitivity g &lt;- watts.strogatz.game(1, 20, 5, 0.05) Directed scale-free graph Get the degree, average path length and transitivity g &lt;- barabasi.game(20) - There should be a power law relation between nodes and degree ... (but this is a very small network) plot(log(1:20),log(degree(g))) "],
["social-networks.html", "10.2 Social Networks", " 10.2 Social Networks The package igraph contains data on a Social network of friendships between 34 members of a karate club at a US university in the 1970s. See W. W. Zachary, An information flow model for conflict and fission in small groups, Journal of Anthropological Research 33, 452-473 (1977). Get a graph of the cpmmunities within the social network. Check the manual, or any online source to figure out what these measures mean and how they are calulated. # Community membership karate &lt;- graph.famous(&quot;Zachary&quot;) wc &lt;- walktrap.community(karate) plot(wc, karate) modularity(wc) membership(wc) It is also possible to get the adjacency matrix get.adjacency(karate) - What do the columns, rows and 0s and 1s stand for? "],
["small-world-index-and-degree-distribution.html", "10.3 Small World Index and Degree Distribution", " 10.3 Small World Index and Degree Distribution Select and run all the code below This will compute the Small World Index and compute the Power Law slope of Small world networks and Scale-free networks Compare the measures! Try to figure out how these graphs are constructed by looking at the functions in nlRtsa_SOURCE.R # Initialize set.seed(660) layout1=layout.kamada.kawai k=3 n=50 # Setup plots par(mfrow=c(2,3)) # Strogatz rewiring probability = .00001 p &lt;- 0.00001 p1 &lt;- plotSW(n=n,k=k,p=p) PLF1&lt;- PLFsmall(p1) p11 &lt;- plot(p1,main=&quot;p = 0.00001&quot;,layout=layout1,xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p1,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p1,N=100)$valuesAV$SWI,digits=2),&quot;\\nPLF = NA&quot;,sep=&quot;&quot;)) # Strogatz rewiring probability = .01 p &lt;- 0.01 p2 &lt;- plotSW(n=n,k=k,p=p) PLF2&lt;- PLFsmall(p2) p22 &lt;- plot(p2,main=&quot;p = 0.01&quot;,layout=layout1,xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p2,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p2,N=100)$valuesAV$SWI,digits=2),&quot;\\nPLF = &quot;,round(PLF2,digits=2),sep=&quot;&quot;)) # Strogatz rewiring probability = 1 p &lt;- 1 p3 &lt;- plotSW(n=n,k=k,p=p) PLF3&lt;- PLFsmall(p3) p33 &lt;- plot(p3,main=&quot;p = 1&quot;,layout=layout1,xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p3,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p3,N=100)$valuesAV$SWI,digits=2),&quot;\\nPLF = &quot;,round(PLF3,digits=2),sep=&quot;&quot;)) set.seed(200) # Barabasi power = 0 p4 &lt;- plotBA(n=n,pwr=0,out.dist=hist(degree(p1,mode=&quot;all&quot;),breaks=(0:n),plot=F)$density) PLF4&lt;- PLFsmall(p4) p44 &lt;- plot(p4,main=&quot;power = 0&quot;,layout=layout1,xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p4,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p4,N=100)$valuesAV$SWI,digits=2),&quot;\\nPLF = &quot;,round(PLF4,digits=2),sep=&quot;&quot;)) # Barabasi power = 2 p5 &lt;- plotBA(n=n,pwr=2,out.dist=hist(degree(p2,mode=&quot;all&quot;),breaks=(0:n),plot=F)$density) PLF5&lt;- PLFsmall(p5) p55 &lt;- plot(p5,main=&quot;power = 2&quot;,layout=layout1,xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p5,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p5,N=100)$valuesAV$SWI,digits=2),&quot;\\nPLF = &quot;,round(PLF5,digits=2),sep=&quot;&quot;)) # Barabasi power = 4 p6 &lt;- plotBA(n=n,pwr=4,out.dist=hist(degree(p3,mode=&quot;all&quot;),breaks=(0:n),plot=F)$density) PLF6&lt;- PLFsmall(p6) p66 &lt;- plot(p6,main=&quot;power = 4&quot;,layout=layout1,xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p6,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p6,N=100)$valuesAV$SWI,digits=2),&quot;\\nPLF = &quot;,round(PLF6,digits=2),sep=&quot;&quot;)) par(mfrow=c(1,1)) | Jump to solutions | "],
["package-qgraph.html", "10.4 Package qgraph", " 10.4 Package qgraph Learn about the functionality of the qgraph frm its author Sacha Epskamp. Try running the examples, e.g. of the Big 5: http://sachaepskamp.com/qgraph/examples "],
["qgraph-tutorials-blog.html", "10.5 qgraph tutorials / blog", " 10.5 qgraph tutorials / blog Great site by Eiko Fried: http://psych-networks.com "],
["extra-early-warnings.html", "10.6 EXTRA - early warnings", " 10.6 EXTRA - early warnings Have a look at the site Early Warning Systems There is an accompanying R package earlywarnings "],
["preparation-prep.html", "PREPARATION {#prep} ", " PREPARATION {#prep} "],
["new-to-r.html", "New to R?", " New to R? You have probably heard many people say they should invest more time and effort to learn to use the R software environment for statistical computing… and they were right. However, what they probably meant to say is: “I tried it, but it’s so damned complicated, I gave up”… and they were right. That is, they were right to note that this is not a point and click tool designed to accommodate any user. It was built for the niche market of scientists who use statistics, but in that segment it’s actually the most useful tool I have encountered so far. Now that your struggles with getting a grip on R are fully acknowledged in advance, let’s try to avoid the ‘giving up’ from happening. Try to follow these steps to get started: Get R and add some user comfort: Install the latest R software and install a user interface like RStudio… It’s all free! An R interface will make some things easier, e.g., searching and installing packages from repositories. RStudio will also add functionality, like git/svn version control, project management and more, like the tools to create html pages like this one (knitr and Rmarkdown). Another source of user comfort are the packages. R comes with some basic packages installed, but you’ll soon need to fit generalised linear mixture models, or visualise social networks using graph theory and that means you’ll be searching for packages that allow you to do such things. A good place to start package hunting are the CRAN task view pages. Learn by running example code: Copy the commands in the code blocks you find on this page, or any other tutorial or help files (e.g., Rob Kabacoff’s Quick R). Paste them into an .R script file in the script (or, source) editor. In RStudio You can run code by pressing cmd + enter when the cursor is on a single single line, or you can run multiple lines at once by selecting them first. If you get stuck remember that there are expert R users who probably have answered your question already when it was posted on a forum. Search for example through the Stackoverflow site for questions tagged with R) Examine what happens… when you tell R to make something happen: R stores variables (anything from numeric data to functions) in an Environment. There are in fact many different environments, but we’ll focus on the main workspace for the current R session. If you run the command x &lt;- 1+1, a variable x will appear in the Environment with the value 2 assigned to it. Examining what happens in the Environment is not the same as examining the output of a statistical analysis. Output in R will appear in the Console window. Note that in a basic set-up each new R session starts with an empty Environment. If you need data in another session, you can save the entire Environment, or just some selected variables, to a file (.RData). Learn about the properties of R objects: Think of objects as containers designed for specific content. One way to characterize the different objects in R is by how picky they are about the content you can assign it. There are objects that hold character and numeric type data, a matrix for numeric data organised in rows and columns, a data.frame is a matrix that allows different data types in columns, and least picky of all is the list object. It can carry any other object, you can have a list of which item 1 is an entire data.frame and item 2 is just a character vector of the letter R. The most difficult thing to master is how to efficiently work with these objects, how to assign values and query contents. Avoid repeating yourself: The R language has some amazing properties that allow execution of many repetitive algorithmic operations using just a few lines of code at speeds up to warp 10. Naturally, you’ll need to be at least half Vulcan to master these features properly and I catch myself copying code when I shouldn’t on a daily basis. The first thing you will struggle with are the apply functions. These functions pass the contents of a list object to a function. Suppose we need to calculate the means of column variables in 40 different SPSS .sav files stored in the folder DAT. With the foreign package loaded we can execute the following commands: data &lt;- lapply(dir(&quot;/DAT/&quot;,pattern=&quot;.sav$&quot;),read.spss) out &lt;- sapply(data,colMeans) The first command applies read.spss to all files with a .sav extension found in the folder /DAT. It creates a dataframe for each file which are all stored as elements of the list data. The second line applies the function colMeans to each element of data and puts the combined results in a matrix with dataset ID as columns (1-40), dataset variables as rows and the calculated column means as cells. This is just the beginning of the R magic, wait ’till you learn how to write functions that can create functions. "],
["not-new-to-r.html", "Not new to R?", " Not new to R? If you have been using R for a while, but do not consider yourself a master yet, I recommend learning to use the tidyverse packages and the accompanying web-book R for data scientists. "],
["the-time-series-object-tsplot.html", "The time series object {#tsPlot}", " The time series object {#tsPlot} The time series object is expected to have a time-dimension on the x-axis. This is very convenient, because R will generate the time axis for you by looking at the time series properties attribute of the object. Even though we are not working with measurement ourcomes, consider a value at a time-index in a time series object a sample: Start - The value of time at the first sample in the series (e.g., \\(0\\), or \\(1905\\)) End - The value of time at the last sample in the series (e.g., \\(100\\), or \\(2005\\)) Frequency - The amount of time that passed between two samples, or, the sample rate (e.g., \\(0.5\\), or \\(10\\)) Examples of using the time series object. Y &lt;- ts(sin(1:100)) plot(Y, type=&quot;l&quot;) # Get sample rate info tsp(Y) ## [1] 1 100 1 # Extract the time vector time(Y) ## Time Series: ## Start = 1 ## End = 100 ## Frequency = 1 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 For now, these values are in principle all arbitrary units (a.u.). These settings only make sense if they represent the parameters of an actual measurement procedure. It is easy to adjust the time vector, by assigning new values using tsp() (values have to be possible given the timeseries length). For example, suppose the sampling frequency was \\(0.1\\) instead of \\(1\\) and the Start time was \\(10\\) and End time was \\(1000\\). # Assign new values tsp(Y) &lt;- c(10, 1000, .1) # Time axis is automatically adjusted time(Y) ## Time Series: ## Start = 10 ## End = 1000 ## Frequency = 0.1 ## [1] 10 20 30 40 50 60 70 80 90 100 110 120 130 140 ## [15] 150 160 170 180 190 200 210 220 230 240 250 260 270 280 ## [29] 290 300 310 320 330 340 350 360 370 380 390 400 410 420 ## [43] 430 440 450 460 470 480 490 500 510 520 530 540 550 560 ## [57] 570 580 590 600 610 620 630 640 650 660 670 680 690 700 ## [71] 710 720 730 740 750 760 770 780 790 800 810 820 830 840 ## [85] 850 860 870 880 890 900 910 920 930 940 950 960 970 980 ## [99] 990 1000 Plotting a ts object as a time series Depending on which packages you use, there will be different settings applied to time series objects created by ts(). Below are some examples of differences between plotting routines. require(lattice) # Needed for plotting require(latticeExtra) # Needed for plotting # stats::plot.ts plot(growth.ac(r = -.9), lwd = 2, main = &quot;stats::plot.ts&quot;) # lattice::xyplot.ts xyplot(growth.ac(r = -.9), lwd = 2, main = &quot;lattice::xyplot.ts&quot;) Plotting multiple time series in one figure Plot multiple timeseries in frames with plot.ts() in package::stats. This function takes a matrix as input, here we use cbind( ... ). # stats::plot.ts plot(cbind(growth.ac(r = 0.9), growth.ac(r = 1.0), growth.ac(r = -0.8) ), yax.flip = TRUE, ann = FALSE, col = &quot;blue&quot;, frame.plot = TRUE) title(main = expression(paste(&quot;Unrestricted Growth: &quot;,Y[t+1]==r*Y[t])), ylab = &quot;| r = -0.8 | r = 1 | r = 0.9 |&quot;, xlab = &quot;time (a.u.)&quot;) Plot multiple timeseries in one graph with ts.plot() in package::graphics. This function can handle multiple ts objects as arguments. # graphics::ts.plot ts.plot(growth.ac(r = 0.9), growth.ac(r = 1), growth.ac(r = -.8), gpars = list(xlab = &quot;time (a.u.)&quot;, ylab = expression(Y(t)), main = expression(paste(&quot;Unrestricted Growth: &quot;,Y[t+1]==r*Y[t])), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;) ) ) legend(70, -0.015, c(&quot;r = 0.9&quot;,&quot;r = 1.0&quot;, &quot;r = -0.8&quot;), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;), merge = TRUE) Use xyplot() in package::lattice to create a plot with panels. The easiest way to do this is to create a dataset in so-called “long” format. This means the variable to plot is in 1 column and other variables indicate different levels, or conditions under which the variable was observed or simulated. Function ldply() is used to generate \\(Y\\) for three different settings of \\(r\\). The values of \\(r\\) are passed as a list and after a function is applied the result is returned as a dataframe. require(plyr) # Needed for function ldply() # Create a long format dataframe for various values for `r` data &lt;- ldply(c(0.9,1,-0.8), function(r){ Y &lt;- as.numeric(growth.ac(r = r)) cbind.data.frame(Y = Y, time = seq_along(Y), r = paste0(&quot;r = &quot;, r)) }) # Plot using the formula interface xyplot(Y ~ time | r, data = data, type = &quot;l&quot;, main = expression(paste(&quot;Unrestricted Growth: &quot;,Y[t+1]==r*Y[t])), scales = c(relation = &quot;free&quot;)) One can also have different panels represent different growth functions. # Create a long format dataframe for combinations of `type` and `r` param &lt;- list(driving = 1.1, damping = 0.1, logistic = 3.6, vanGeert = 2.0) # Use the `names()` function to pass the `type` string as an argument. data &lt;- ldply(seq_along(param), function(p){ cbind.data.frame(Y = as.numeric(growth.ac(r = param[[p]], type = names(param[p]))), time = as.numeric(time(growth.ac(r = param[[p]], type = names(param[p])))), type = paste0(names(param[p]), &quot; | r = &quot;, param[p])) }) # Plot using the formula interface xyplot(Y ~ time | factor(type), data = data, type = &quot;l&quot;, scales = c(relation = &quot;free&quot;), main = &quot;Four Autocatalytic Growth Models&quot;) A popular package for creating graphs is ggplot2 the code is very similar to lattice, in my opinion ggplot2 will be more flexible and user friendly when your plots become more complex. library(ggplot2) # Create a long format dataframe for combinations of `type` and `r` param &lt;- list(driving = 1.1, damping = 0.1, logistic = 3.6, vanGeert = 2) # Use the `names()` function to pass the `type` string as an argument. data &lt;- ldply(seq_along(param), function(p){ Y = as.numeric(growth.ac(r = param[[p]], type = names(param[p]))) cbind.data.frame(Y = Y, time = seq_along(Y), type = paste0(names(param[p]), &quot; | r = &quot;, param[p]) ) }) # Plot using the formula interface ggplot(data, aes(x= time, y = Y)) + geom_path() + facet_wrap(~type, scales = &quot;free&quot;) + labs(title = &quot;Four Autocatalytic Growth Models&quot;) -->"],
["part-i.html", "PART I ", " PART I "],
["modelling-nonlinear-growth-and-deterministic-chaos.html", "Modelling (nonlinear) growth and Deterministic Chaos", " Modelling (nonlinear) growth and Deterministic Chaos The simplest non-trivial iterative change process can be described by the following difference equation: \\[\\begin{equation} Y_{t+1} = Y_{t=0} + a*Y_t \\tag{10.1} \\end{equation}\\] Equation (10.1) describes the way in which the value of \\(Y\\) changes between two adjacent, discrete moments in time (hence the term difference equation, or recurrence relation). There are two parameters resembling an intercept and a slope: The starting value \\(Y_0\\) at \\(t=0\\), also called the starting value, or the initial conditions. A rule for incrementing time, here the change in \\(Y\\) takes place over a discrete time step of 1: \\(t+1\\). The values taken on by variable \\(Y\\) are considered to represent the states quantifiable observable leAlternative ways to describe the change of states : A dynamical rule describing the propagation of the states of a system observable measured by the values of variable Y through discrete time. A dynamic law describing the time-evolution of the states of a system observable measured by the variable Y. These descriptions all refer to the change processes that govern system observables (properties of dynamical systems that can be observed through measurement). It’s a line! It’s a plane! The formula resembles the equation of a line. There is a constant value \\(Y_{0}\\) which is added to a proportion of the value of \\(Y\\) at time \\(t\\), given by parameter \\(a\\). This is equivalent to the slope of a line. However, in a \\((X,Y)\\) plane there are two ‘spatial’ (metric) dimensions representing the values two variables \\(X\\) and \\(Y\\) can take on (see figure). The best fitting straight line would be called a statistical model of the linear relationship between the observed values of \\(X\\) and \\(Y\\). It can be obtained by fitting a General Linear Model (GLM) to the data. If \\(X\\) were to represent repeated measurements the multivariate GLM for repeated measures would have to be fitted to the data. This can be very problematic, because statistical models rely on Ergodic theory: “… it is the study of the long term average behavior of systems evolving in time.”2 need to assume independence of measurements within and between subjects. These assumptions can be translated to certain conditions that must hold for the model to be valid, known as Compound Symmetry and Sphericity: The compound symmetry assumption requires that the variances (pooled within-group) and covariances (across subjects) of the different repeated measures are homogeneous (identical). This is a sufficient condition for the univariate F test for repeated measures to be valid (i.e., for the reported F values to actually follow the F distribution). However, it is not a necessary condition. The sphericity assumption is a necessary and sufficient condition for the F test to be valid; it states that the within-subject “model” consists of independent (orthogonal) components. The nature of these assumptions, and the effects of violations are usually not well-described in ANOVA textbooks;3 As you can read in the quoted text above, these conditions must hold in order to be able to identify unique independent components as the sources of variation of \\(Y\\) over time within a subject. This is the a clear example of: It is the theory that decides what we may observe4 If you choose to use GLM repeated measures to model change over time, you will only be able to infer independent components that are responsible for the time-evolution of \\(Y\\). As is hinted in the last sentence of the quote, the validity of such inferences is not a common topic of discussion statistics textbooks. No! … It’s a time series! The important difference between a regular 2-dimensional Euclidean plane and the space in which we model change processes is that the \\(X\\)-axis represents the physical dimension time. In the case of the Linear Map we have a 1D space with one ‘spatial’ dimension \\(Y\\) and a time dimension \\(t\\). This is called time series if \\(Y\\) is sampled as a continuous process, or a trial series if the time between subsequent observations is not relevant, just the fact that there was a temporal order (for example, a series of response latencies to trials in a psychological experiment in the order in which they were presented to the subject). Time behaves different from a spatial dimension in that it is directional (time cannot be reversed), it cannot take on negative values, and, unless one is dealing with a truly random process, there will be a temporal correlation across one or more values of \\(Y\\) seperated by an amount of time. In the linear difference equation this occurs because each value one step in the future is calculated based on the current value. If the values of \\(Y\\) represent an observable of a dynamical system, the system can be said to have a history, or a memory. Ergodic systems do not have a history or a memory that extends across more than one time step. This is very convenient, because one can calculate the expected value of a system observable given infinite time, by making use of of the laws of probabilities of random events (or random fields). This means: The average of an observable of an Ergodic system measured across infinite time (its entire history, the time-average), will be the be the same value as the average of this observable measured at one instance in time, but in an infinite amount of systems of the same kind (the population, the spatial average).5 The simple linear difference equation will have a form of *perfect memory’ across the smallest time scale (i.e., the increment of 1, \\(t+1\\)). This ‘memory’ concerns a correlation of 1 between values at adjacent time points (a short range temporal correlation, SRC), because the change from \\(Y_t\\) to \\(Y_{t+1}\\) is exactly equal to \\(a * Y_t\\) at each iteration step. This is the meaning of deterministic, not that each value of \\(Y\\) is the same, but that the value of \\(Y\\) now can be perfectly explained form the value of \\(Y\\) one moment in the past. Summarising, the most profound difference is not the fact that the equation of linear change is a deterministic model and the GLM is a probabilistic model with parameters fitted from data, this is something we can (and will) do for \\(a\\) as well. The profound difference between the models is the role given to the passage of time: The linear difference equation represents changes in \\(Y\\) as a function of the physical dimension time and \\(Y\\) itself. The GLM represents changes in \\(Y\\) as a function of a linear predictor composed of additive components that can be regarded as independent sources of variation that sum up to the observed values of \\(Y\\). See Dajani &amp; Dirksin (2008, p. 5, “A simple introduction to Ergodic Theory”)↩ Retreived from www.statsoft.com↩ Einstein as quoted by Heisenberg.↩ In other words: If you throw 1 die 100 times in a row, the average of the 100 numbers is the time-average of one of the observables of die-throwing systems. If this system is ergodic, then its time-average is expected to be similar to the average of the numbers that turn up if you throw 100 dice all at the same instance of time. The dice layed out on the table represent a spatial sample, a snapshot frozen in time, of the possible states the system can be in. Taking the average would be the spatial average this observable of die-throwing systems. This ergodic condiciotn is often implicitly assumed in Behavioural Science when studies claim to study change by taking different samples of individuals (snapshots of system states) and comparing if they are the same.↩ "],
["numerical-integration-to-simulate-continuous-time.html", "Numerical integration to simulate continuous time", " Numerical integration to simulate continuous time In order to ‘solve’ a differential equation for continuous time using a method of numerical integration, one could code it like in the spreadsheet assignment below. For R and Matlab there are so-called solvers available, functions that will do the integration for you. For R look at the Examples in package deSolve. Euler’s method and more… The result of applying a method of numerical integration is called a numerical solution of the differential equation. The analytical solution is the equation which will give you a value of \\(Y\\) for any point in time, given an initial value \\(Y_0\\). Systems which have an analytical solution can be used to test the accuracy of numerical solutions. Analytical solution Remember that the analytical solution for the logistic equation is: \\[ Y(t) = \\frac{K}{1 + \\left(\\frac{K}{Y_{0} - 1}\\right) * e^{-r*t} } \\] If we want to know the growth level \\(Y_t\\) at \\(t=10\\), with \\(Y_0=.0001\\), \\(r=1.1\\) and \\(K=4\\), we can just fill it in: # Define a function for the solution logSol &lt;- function(Y0, r, K, t){K/(1+(K/Y0-1)*exp(-r*t))} # Call the function logSol(Y0=.0001, r=1.1, K=4, t=10) ## [1] 2.398008 We can pas a vector of timepoints to create the exact solution, the same we would get if we were to iterate the differential/difference equation. # Plot from t=1 to t=100 plot(logSol(Y0=.0001, r=1.1, K=4, t=seq(1,20)), type = &quot;b&quot;, ylab = expression(Y[t]), xlab = &quot;t&quot;) # Plot t=10 in red points(10,logSol(Y0=.0001, r=1.1, K=4, t=10), col=&quot;red&quot;, pch=16) Numerical solution (discrete) If we would iterate the differential equation … \\[ \\frac{dY}{dt} = Y_t * (1 + r - r * \\frac{Y_t}{K}) \\] … as if it were a difference equation, that is, not simulating continuous time. logIter &lt;- function(Y0,r,K,t){ N &lt;- length(t) Y &lt;- as.numeric(c(Y0, rep(NA,N-2))) sapply(seq_along(Y), function(t){ Y[[t+1]] &lt;&lt;- Y[t] * (1 + r - r * Y[t] / K)}) } # Plot from t=1 to t=100 plot(logIter(Y0=.0001, r=1.1, K=4, t=seq(1,20)), type = &quot;b&quot;, ylab = expression(Y[t]), xlab = &quot;t&quot;) # Plot t=10 in red points(10,logSol(Y0=.0001, r=1.1, K=4, t=10), col=&quot;red&quot;, pch=16) "],
["advmodels.html", "10.7 Modeling interactions between processes and agents", " 10.7 Modeling interactions between processes and agents The Competetive Lottka-Volterra Equations The coupled predator-prey dynamics in the previous assignment are not a very realistic model of an actual ecological system. Both equations are exponential growth functions, but Rabbits for example, also have to eat! One way to increase realism is to consider coupled logistic growth by introducing a carrying capacity. Follow the link to the Wiki page and try to model the system! This is what interaction dynamics refers to, modeling mutual dependiencies using the if ... then conditional rules isn’t really about interaction, or coupling between processes. Predator-Prey (and other) dynamics as Agent Based Models Agent-Based models are an expansion of the idea of “connected growers” that includes a spatial location of the things that is subject to change over time. Have a look at some of the NETlogo demo’s: Rabbits Weeds Grass Wolf Sheep Grass -->"],
["part-ii.html", "PART II", " PART II It will become increasingly difficult to use software like Excel and SPSS. Perhaps now is a good time to switch to R or Matlab. We do have a spreadsheet example of Standardised Dispersion Anaysis. Using R: Install functions in nlRtsa_SOURCE.R First, download (from blackboard) and source('nlRtsa_SOURCE.R'), or source it directly from Github if you have package devtools installed. library(devtools) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) We need packages signal and pracma Among nlRtsa functions is in.IT(), which will load a list of packages and install them, but only if they are not present on your system. in.IT(c(&quot;signal&quot;,&quot;pracma&quot;)) You can of course also use install.packages() or the GUI. "],
["examples-fast-fourier-transform-and-power-spectrum.html", "Examples: Fast Fourier transform and Power Spectrum", " Examples: Fast Fourier transform and Power Spectrum Below is an example of a signal built from sine components (y) whose relative amplitudes are recovered in the powerspectrum. The amplitudes are differently scaled sinewaves which are summed or subtracted form one another. # Sawtooth x &lt;- seq(-3.2,3.2, length.out = 256) y &lt;- 2*sin(10*x) - 1*sin(20*x) + (2/3)*sin(30*x) - (1/2)*sin(40*x) + (2/5)*sin(50*x) - (1/4)*sin(60*x) # Plot the sawtooth wave as constructed by the Fourier series above plot(x,y, xlab =&#39;Time (a.u.)&#39;, ylab = &#39;Variable (a.u.)&#39;, main =&#39;Sawtooth wave&#39;, type = &quot;l&quot;) # Perform a Fast Fourier Transform and calculate the Power and Frequency (you don&#39;t have to know how this works) Y &lt;- fft(y) Pyy &lt;- Y*Conj(Y)/256 f &lt;- 1000/256*(0:127) # Plot the power spectrum of the sawtooth wave plot(f[1:50],Pyy[1:50], type=&quot;b&quot;,xlab=&#39;Frequency (a.u.)&#39;, ylab =&#39;Power (a.u.)&#39;, pch=21, bg=&#39;grey60&#39;, main = &#39;Power Spectrum&#39;) The \\(6\\) peaks with an amplitude &gt; \\(0\\) are the \\(6\\) sine components used to construct the signal: + 2 *sin(10*x) - 1 *sin(20*x) + (2/3)*sin(30*x) - (1/2)*sin(40*x) + (2/5)*sin(50*x) - (1/4)*sin(60*x) Now we do the same for a very noisy signal into which we insert one dominant frequency and two smaller ones. # A time vector t &lt;- pracma::linspace(x1 = 0, x2 = 50, n = 256) # There are three sine components x &lt;- sin(2*pi*t/.1) + sin(2*pi*t/.3) + sin(2*pi*t/.5) # Add random noise! y &lt;- x + 1*randn(size(t)) # Plot the noise. plot(t, y, type = &quot;l&quot;, xlab = &#39;Time (a.u.)&#39;, ylab = &#39;Variable (a.u.)&#39;, main = &#39;A very noisy signal&#39;) # Get the frequency domain Y &lt;- fft(y) Pyy &lt;- Y*Conj(Y)/256 f &lt;- 1000/256*(0:127) # Plot the power spectrum of this noise plot(f[1:50],Pyy[1:50], type=&quot;b&quot;,xlab=&#39;Frequency (a.u.)&#39;, ylab=&#39;Power (a.u.)&#39;, pch=21, bg=&#39;grey60&#39;, main = &#39;Power Spectrum&#39;) The \\(3\\) peaks with an amplitude &gt; \\(0\\) are the \\(3\\) sine components used to construct the signal to which random noise was added: + sin(2*pi*t/.1) + sin(2*pi*t/.3) + sin(2*pi*t/.5) More information about the Fourier transform and how to use R functions. "],
["data-considerations.html", "Data considerations", " Data considerations “If you have not found the fractal pattern, you have not taken enough data” – Machlup, 1977) All analyses: Data points: \\(2^n\\), minimum 1024 (\\(2^{10}\\)) Remove 3SD only if this is absolutely necessary Spectral analysis: Normalize the data (z-score transform: (X-mean(X))/SD(X) Remove linear trend if necessary (detrend) Decide number of frequencies to estimate, min. 512 SDA: Normalize the data (z-score transform: (X-mean(X))/SD(X) DFA: Nothing extra, analysis integrates and detrends the signal -->"],
["part-iii.html", "PART III ", " PART III "],
["creating-fractals-from-random-processes.html", "10.8 Creating fractals from random processes", " 10.8 Creating fractals from random processes Below are examples of so-called Iterated Function Systems, copy the code and run it in R Try to understand what is going on in the two examples below. - How does the structure come about? We are drawing random numbers! - What’s the difference between the Siepinsky Gasket and the Fern? A Triangle # Sierpinski Gasket using Iterated Function Systems # # RM-course Advanced Data Analysis # Module Dynamical and Nonlinear Data analysis and Modeling # # May 2008 # Fred Hasselman &amp; Ralf Cox require(dplyr) x = 0 # Starting points y = 0 # Emppty plot plot(x,y, xlim=c(0,2), ylim=c(0,1)) for(i in 1:20000){ # This takes some time: 20.000 iterations coor=runif(1) # coor becomes a random number between 0 and 1 drawn from the uniform distribution # Equal chances (33%) to perform one of these 3 transformations of x and y if(coor&lt;=0.33){ x=0.5*x y=0.5*y points(x,y,pch=&quot;.&quot;, col=&quot;green&quot;) #plot these points in green } if(between(coor,0.33,0.66)){ x=0.5*x+0.5 y=0.5*y+0.5 points(x,y, pch=&quot;.&quot;, col=&quot;blue&quot;) # plot these points in blue } if(coor&gt;0.66){ x=0.5*x+1 y=0.5*y points(x,y, pch=&quot;.&quot;, col=&quot;red&quot;) #plot these points in red } } # for ... A Fern # Barnsley&#39;s Fern using Iterated Function Systems # # RM-course Advanced Data Analysis # Module Dynamical and Nonlinear Data analysis and Modeling # # May 2008 # Fred Hasselman &amp; Ralf Cox require(dplyr) x = 0 # Starting points y = 0 # Emppty plot plot(x,y, pch=&quot;.&quot;, xlim=c(-3,3), ylim=c(0,12)) for(i in 1:50000){ # This takes some time: 20.000 iterations coor=runif(1) # coor becomes a random number between 0 and 1 drawn from the uniform distribution if(coor&lt;=0.01){ #This transformation 1% of the time x = 0 y = 0.16 * y points(x,y, pch=&quot;.&quot;, col=&quot;green3&quot;) } if(between(coor,0.01, 0.08)){ #This transformation 7% of the time x = 0.20 * x - 0.26 * y y = 0.23 * x + 0.22 * y + 1.6 points(x,y, pch=&quot;.&quot;, col=&quot;green2&quot;) } if(between(coor,0.08,0.15)){ #This transformation 7% of the time x = -0.15 * x + 0.28 * y y = 0.26 * x + 0.24 * y + 0.44 points(x,y, pch=&quot;.&quot;, col=&quot;springgreen3&quot;) } if(coor&gt;0.15){ #This transformation 85% of the time x = 0.85 * x + 0.04 * y y= -0.04 * x + 0.85 * y + 1.6 points(x,y, pch=&quot;.&quot;, col=&quot;springgreen2&quot;) } } # for ... The fractal / chaos game These Iterated Function Systems also go by the name of ‘the fractal game’ and are used in computer science, the gaming industry, graphic design, etc. EXTRA-EXTRA: This Wikipedia page on Barnsley’s fern has some good info on the topic. At the end they display Mutant varieties. Try to implement them! You can by now probably guess that the these simple rules can be described as constraints on the degrees of freedom of the system. Like with the models of growth we simulated, the rules of the fractal game can be made dependent on other processes or events. A great example are the so-called fractal flames implemented in a screen-saver called electric sheep, which combines genetic algorithms, distributed computind and user input (“likes”) to create intruiging visual patterns on your computer screen.6 --> Use at your own risk! You will find yourself silently staring at the screen for longer periods of time.↩ "],
["lecture-6.html", "Lecture 6", " Lecture 6 "],
["lecture-7.html", "Lecture 7", " Lecture 7 "],
["lecture-8.html", "Lecture 8", " Lecture 8 "],
["lecture-9.html", "Lecture 9", " Lecture 9 "],
["mathematics-of-change-i.html", "A Mathematics of change I", " A Mathematics of change I Solutions to assignments in section 2. Linear and logistic growth Deterministic Chaos "],
["linear-and-logistic-growth.html", "A.1 Linear and logistic growth", " A.1 Linear and logistic growth Solutions in a spreadsheet The ‘solutions’ to iterating the Linear Map and theLogistic Map in a spreadsheet can be found in this GoogleSheet. Solutions in R | jump to question | Coding the difference equations in Matlab and R is always easier than using a spreadsheet. One obvious way to do it is to use a counter variable representing the iterations of time in a for ... next loop. The iterations should run over a vector (which is the same concept as a row or a column in a spreadsheet: An indexed array of numbers or characters). The first entry should be the starting value, so the vector index \\(1\\) represents \\(Y_0\\). The loop can be implemented a number of ways, for example as a function which can be called from a script or the command / console window. In R working with functions is easy, and very much recommended, because it will speed up calculations considerably, and it will reduce the amount of code you need to write. You need to gain some experience with coding in R before you’ll get it right. In order to get it lean and clean (and possibly even mean as well) you’ll need a lot of experience with coding in R,therefore, we will (eventually) provide you the functions you’ll need to complete the assignments. All you have to do is figure out how to use, or modify them to suit your specific needs. To model the autocatalytic growth equations we provide a function growth.ac(), which is able to simulate all of the processes discussed in the lectures. Using just a few lines of code, each of the 4 difference equations used in the assignments can be simulated. Basically the code block below contains the solutions to the Linear Map, the stylized Logisitc Map and the Van Geert model for cognitive growth. library(plyr) growth.ac &lt;- function(Y0 = 0.01, r = 1, k = 1, N = 100, type = c(&quot;driving&quot;, &quot;damping&quot;, &quot;logistic&quot;, &quot;vanGeert&quot;)[1]){ # Create a vector Y of length N, which has value Y0 at Y[1] if(N&gt;1){ Y &lt;- as.numeric(c(Y0, rep(NA,N-2))) # Conditional on the value of type ... switch(type, # Iterate N steps of the difference function with values passed for Y0, k and r. driving = sapply(seq_along(Y), function(t) Y[[t+1]] &lt;&lt;- r * Y[t] ), damping = k + sapply(seq_along(Y), function(t) Y[[t+1]] &lt;&lt;- Y[t] -r * Y[t] / k), logistic = sapply(seq_along(Y), function(t) Y[[t+1]] &lt;&lt;- r * Y[t] * ((k - Y[t]) / k)), vanGeert = sapply(seq_along(Y), function(t) Y[[t+1]] &lt;&lt;- Y[t] * (1 + r - r * Y[t] / k)) )} return(ts(Y)) } # Call the function with default settings and r = 1.1 Y &lt;- growth.ac(r = 1.1) Some notes about this function: To select which growth process to simulate, the argument type is defined which takes the values driving (default), damping, logistic and vanGeert. The statement switch(type, ...) will iterate an equation based on the value of type. A time series object is returned due to the function ts(). This is a convenient way to represent time series data, it can also store the sample rate of the signal and start and end times. Most of the basic functions, like plot() and summary() will recognise a time series object when it is passed as an argument and use settings appropriate for time series data. The sapply() function iterates \\(t\\) from \\(1\\) to the number of elements in \\(Y\\) (seq_along(Y)) and then applies the function. The double headed arrow &lt;&lt;- is necessary because we want to update vector \\(Y\\), which is defined outside the sapply() environment. The return plot To create a return plot the values of \\(Y\\) have to be shifted by a certain lag. The functions lead() and lag() in package::dplyr are excellent for this purpose (note that dplyr::lag() behaves different from stats::lag()). # Function lag() and lead() require(dplyr) # Get exponential growth Y1 &lt;- growth.ac(Y0 = .9, r = .9, N = 1000, type = &quot;driving&quot;) # Get logistic growth in the chaotic regime Y2 &lt;- growth.ac(r = 4, N = 1000, type = &quot;logistic&quot;) # Use the `lag` function from package `dplyr` op &lt;- par(mfrow = c(1,2), pty = &quot;s&quot;) plot(lag(Y1), Y1, xy.labels = FALSE, pch = &quot;.&quot;, xlim = c(0,1), ylim = c(0,1), xlab = &quot;Y(t)&quot;, ylab = &quot;Y(t+1)&quot;, main = expression(paste(Y[t+1]==r*Y[t]))) plot(lag(Y2), Y2, xy.labels = FALSE, pch = &quot;.&quot;, xlim = c(0,1), ylim = c(0,1), xlab = &quot;Y(t)&quot;, ylab = &quot;Y(t+1)&quot;, main = expression(paste(Y[t+1]==r*Y[t]*(1-Y[t])))) par(op) Use l_ply() from package::plyr to create return plots with different lags. The l_ before ply means the function will take a list as input to a function, but it will not expect any data to be returned, for example in the case of a function that is used to plot something. # Explore different lags op &lt;- par(mfrow = c(1,2), pty = &quot;s&quot;) l_ply(1:4, function(l) plot(lag(Y2, n = l), Y2, xy.labels = FALSE, pch = &quot;.&quot;, xlim = c(0,1), ylim = c(0,1), xlab = &quot;Y(t)&quot;, ylab = paste0(&quot;Y(t+&quot;,l,&quot;)&quot;), cex = .8)) par(op) -->"],
["extra.html", "B EXTRA", " B EXTRA Solutions to assignments in section 3 Time-varying parameters Predator-prey dynamics "],
["tvpar.html", "B.1 Time-varying parameters", " B.1 Time-varying parameters Solutions in a spreadsheet Van Geert, including jumps and stages. B.1.1 Solutions in R The growth model by Van Geert (1991) Different values for r: # Parameters rs &lt;- c(1.2, 2.2, 2.5, 2.7, 2.9, 3) # Plot op &lt;- par(mfrow=c(1,2)) l_ply(rs,function(r){plot(growth.ac(r = r, Y0 = 0.01, type = &quot;vanGeert&quot;), ylim = c(0,1.4), ylab = &quot;L(t)&quot;, main = paste(&quot;r =&quot;,r))}) par(op) Different values for \\(k\\) reveal that the dispersion of values (variance) increases if the carrying capacity increases. This occurs because we are dealing with nonlinear changes to the values of \\(Y\\) and if larger values of \\(Y\\) are allowed by a hihger \\(k\\), these values will be amplified once they occur. # Parameters ks &lt;- c(0.5, 0.75, 1, 1.5) # Plot op &lt;- par(mfrow=c(1,2)) l_ply(ks,function(k){plot(growth.ac(r = 2.9, k = k, Y0 = 0.01, type = &quot;vanGeert&quot;), ylim = c(0, 2), ylab = &quot;L(t)&quot;, main = paste(&quot;k =&quot;,k))}) par(op) Stages and Jumps library(lattice) growth.ac.cond &lt;- function(Y0 = 0.01, r = 0.1, k = 2, cond = cbind.data.frame(Y = 0.2, par = &quot;r&quot;, val = 2), N = 100){ # Create a vector Y of length N, which has value Y0 at Y[1] Y &lt;- c(Y0, rep(NA, N-1)) # Iterate N steps of the difference equation with values passed for Y0, k and r. cnt &lt;- 1 for(t in seq_along(Y)){ # Check if the current value of Y is greater than the threshold for the current conditional rule in cond if(Y[t] &gt; cond$Y[cnt]){ # If the threshold is surpassed, change the parameter settings by evaluating: cond$par = cond$val eval(parse(text = paste(cond$par[cnt], &quot;=&quot;, cond$val[cnt]))) # Update the counter if there is another conditional rule in cond if(cnt &lt; nrow(cond)){cnt &lt;- cnt + 1} } # Van Geert growth model Y[[t+1]] &lt;- Y[t] * (1 + r - r * Y[t] / k) } return(ts(Y)) } # Plot with the default settings (same as first step in the assignment) xyplot(growth.ac.cond()) The ‘trick’ used here is to define the function such that it can take a set of conditional rules and apply them sequentially during the iterations. The conditiona rule is passed as a data.frame, but one could also use a list object. (cond &lt;- cbind.data.frame(Y = c(0.2, 0.6), par = c(&quot;r&quot;, &quot;r&quot;), val = c(0.5, 0.1))) Y par val 1 0.2 r 0.5 2 0.6 r 0.1 xyplot(growth.ac.cond(cond=cond)) Or, combine a change of r and a change of k (cond &lt;- cbind.data.frame(Y = c(0.2, 1.99), par = c(&quot;r&quot;, &quot;k&quot;), val = c(0.5, 3))) Y par val 1 0.20 r 0.5 2 1.99 k 3.0 xyplot(growth.ac.cond(cond=cond)) # A fantasy growth process (cond &lt;- cbind.data.frame(Y = c(0.1, 1.99, 1.999, 2.5, 2.9), par = c(&quot;r&quot;, &quot;k&quot;, &quot;r&quot;, &quot;r&quot;,&quot;k&quot;), val = c(0.3, 3, 0.9, 0.1, 1.3))) Y par val 1 0.100 r 0.3 2 1.990 k 3.0 3 1.999 r 0.9 4 2.500 r 0.1 5 2.900 k 1.3 xyplot(growth.ac.cond(cond=cond)) Connected Growers Somewhat more realstic would be to model a change of r as dependent on the values of another process. The proper ‘dynamical’ way to do this would be to define a coupled system of difference or differential equations in which the interaction dynamics regulate growth. An example is the predator-prey system discussed in the next assignment. Using the ‘conditional’ rules on a number of seperate processes will ‘work’ as a model, but it isn’t exactly what is meant by interaction dynamics, or multiplicative interactions. Basically, these processes will be independent and non-interacting. The conditional rules that change the parameters are ‘given’. # Generate 3 timeseries Y1 &lt;- growth.ac(k = 2, r =.2, type = &quot;vanGeert&quot;) # Y2 and Y3 start at r = 0.001 Y3 &lt;- Y2 &lt;- growth.ac(k = 2, r = 0.001, type = &quot;vanGeert&quot;) # Y2 and Y3 start when k is approached c1 &lt;- 1.6 c2 &lt;- 2.2 Y2[Y1 &gt; c1] &lt;- growth.ac(r = .3, k = 3, type = &quot;vanGeert&quot;, N = sum(Y1 &gt; c1)) Y3[Y2 &gt; c2] &lt;- growth.ac(r = .5, k = 4, type = &quot;vanGeert&quot;, N = sum(Y2 &gt; c2)) # Make a nice plot ts.plot(Y1, Y2, Y3, gpars = list(xlab = &quot;time (a.u.)&quot;, ylab = expression(Y(t)), main = expression(paste(&quot;&#39;Connected&#39; Growers &quot;,Y[t+1]==Y[t]*(1 + r - r*Y[t]))), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;) ) ) legend(1, 3.8, c(&quot;Y1(0): r = .2&quot;, paste0(&quot;Y2(&quot;,which(Y1 &gt; c1)[1],&quot;): r = .3&quot;), paste0(&quot;Y3(&quot;,which(Y2 &gt; c2)[1],&quot;): r = .5&quot;)), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;), merge = TRUE) -->"],
["ppdsol.html", "C Predator-prey dynamics", " C Predator-prey dynamics | jump to question | Iterating 2D Maps and Flows In order to ‘solve’ a differential equation for time using a method of numerical integration, one could code it like in the spreadsheet assignment. For R and Matlab there are so-called solvers available, functions that will do the integration for you. Look at the Examples in package deSolve. "],
["solutions-in-a-spreadsheet-2.html", "Solutions in a spreadsheet", " Solutions in a spreadsheet Predator-Prey Dynamics "],
["solutions-in-r-2.html", "C.1 Solutions in R", " C.1 Solutions in R Euler’s method and more… The result of applying a method of numerical integration is called a numerical solution of the differential equation. The analytical solution is the equation which will give you a value of \\(Y\\) for any point in time, given an initial value \\(Y_0\\). Systems which have an analytical solution can be used to test the accuracy of numerical solutions. Remember that the analytical solution for the logistic equation is: \\[\\begin{equation} \\frac{K}{1 + \\left(\\frac{K}{Y_0 - 1}\\right) * e^{-r*t} } \\tag{C.1} \\end{equation}\\] We have the function growth.ac() and could easily adapt all the functions to use Euler’s method. Below is a comparison of the analytic solution with Euler’s method. # Parameter settings d &lt;- 1 N &lt;- 100 r &lt;- .1 k &lt;- 1 Y0 &lt;- 0.01 Y &lt;- as.numeric(c(Y0, rep(NA,N-1))) # Numerical integration of the logistic differential equation Y.euler1 &lt;- ts( sapply(seq_along(Y), function(t) Y[[t+1]] &lt;&lt;- (r * Y[t] * (k - Y[t])) * d + Y[t] )) Y.euler2 &lt;- ts( sapply(seq_along(Y), function(t) Y[[t+1]] &lt;&lt;- (r * Y[t] * (k - Y[t])) * (d+.1) + Y[t] )) ## analytical solution Y.analytic &lt;- ts( k / (1 + (k / Y0 - 1) * exp(-r*(time(Y.euler1)))) ) ts.plot(Y.analytic, Y.euler1, Y.euler2, gpars = list(xlab = &quot;time (a.u.)&quot;, ylab = expression(Y(t)), main = expression(paste(&quot;Analytic vs. Numerical:&quot;,Y[t+1]==Y[t]*(1 + r - r*Y[t]))), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;) ) ) legend(50, 0.4, c(&quot;Analytic&quot;, &quot;Euler: delta = 1.0&quot;, &quot;Euler: delta = 1.1&quot;), lwd = rep(2,3), lty = c(1:3), col = c(&quot;darkred&quot;,&quot;darkblue&quot;,&quot;darkgreen&quot;), merge = TRUE) Numerical integration The Euler setup: \\[\\begin{align} R_{t+1} &amp;= f_R(R_t,Ft) * \\Delta + R_t \\\\ F_{t+1} &amp;= f_F(R_t,F_t) * \\Delta + F_t \\end{align}\\] With the equations: \\[\\begin{align} R_{t+1} &amp;= (a-b*F_t)*R_t * \\Delta + R_t \\\\ \\\\ F_{t+1} &amp;= (c*R_t-d)*F_t * \\Delta + F_t \\end{align}\\] # Parameters N &lt;- 1000 a &lt;- d &lt;- 1 b &lt;- c &lt;- 2 R0 &lt;- F0 &lt;- 0.1 R &lt;- as.numeric(c(R0, rep(NA,N-1))) F &lt;- as.numeric(c(F0, rep(NA,N-1))) # Time constant delta &lt;- 0.01 # Numerical integration of the logistic differential equation l_ply(seq_along(R), function(t){ R[[t+1]] &lt;&lt;- (a - b * F[t]) * R[t] * delta + R[t] F[[t+1]] &lt;&lt;- (c * R[t] - d) * F[t] * delta + F[t] }) # Note different behaviour when ts() is applied xyplot(cbind(ts(R),ts(F))) xyplot(R ~ F, pch = 16) -->"],
["btasol.html", "D Basic Timeseries Analysis ", " D Basic Timeseries Analysis "],
["nonlinear-growth-curves-in-spss.html", "D.1 Nonlinear Growth curves in SPSS", " D.1 Nonlinear Growth curves in SPSS | Jump to question | The solutions are provided as an SPSS syntax file file. Or copy the block below: GRAPH /LINE(SIMPLE)=VALUE(Yt) BY Time. * NonLinear Regression. MODEL PROGRAM Yzero=0.01 r=0.01 K=0.01. COMPUTE PRED_=K*Yzero/(Yzero + (K-Yzero) * EXP(-1*(r * K * Time))). NLR Yt /PRED PRED_ /SAVE PRED /CRITERIA SSCONVERGENCE 1E-8 PCON 1E-8. GRAPH /LINE(MULTIPLE)=VALUE(Yt PRED_) BY Time. COMPUTE T1=Yt * Time. COMPUTE T2=Yt * (Time ** 2). COMPUTE T3=Yt * (Time ** 3). COMPUTE T4=Yt * (Time ** 4). EXECUTE. REGRESSION /MISSING LISTWISE /STATISTICS COEFF OUTS R ANOVA /CRITERIA=PIN(.05) POUT(.10) /NOORIGIN /DEPENDENT Yt /METHOD=ENTER T1 T2 T3 T4 /SAVE PRED. GRAPH /LINE(MULTIPLE)=VALUE(Yt PRED_ PRE_1) BY Time. The point here is that the polynomial regression appoach is “just” curve fitting … adding components until a nice fit is found … but what does component \\(Y_t^4\\) represent? A quartic subprocess? Fitting the solution of the the logistic function will give us parameters we can interpret unambiguoulsy: Carrying capacity, growth rate, starting value. "],
["pacfsol.html", "D.2 Correlation functions and AR-MA models", " D.2 Correlation functions and AR-MA models | Jump to question | The solutions are provided as an SPSS syntax file file. Or copy the block below: DESCRIPTIVES VARIABLES=TS_1 TS_2 TS_3 /STATISTICS=MEAN STDDEV MIN MAX . *Sequence Charts . TSPLOT VARIABLES= TS_1 /NOLOG /FORMAT NOFILL REFERENCE. TSPLOT VARIABLES= TS_2 /NOLOG /FORMAT NOFILL REFERENCE. TSPLOT VARIABLES= TS_3 /NOLOG /FORMAT NOFILL REFERENCE. *ACF and PCF. ACF VARIABLES= TS_1 TS_2 TS_3 /NOLOG /MXAUTO 30 /SERROR=IND /PACF. * ARIMA with p=5 and q=1. TSET PRINT=DEFAULT CIN=95 NEWVAR=ALL . PREDICT THRU END. ARIMA TS_2 /MODEL=( 5 0 1)CONSTANT /MXITER= 10 /PAREPS= .001 /SSQPCT= .001 /FORECAST= EXACT . * ARIMA with p=2 and q=1. TSET PRINT=DEFAULT CIN=95 NEWVAR=ALL . PREDICT THRU END. ARIMA TS_2 /MODEL=( 2 0 1)CONSTANT /MXITER= 10 /PAREPS= .001 /SSQPCT= .001 /FORECAST= EXACT . *Plot Fit. GRAPH /LINE(MULTIPLE)=MEAN(TS_2) MEAN(FIT_2) MEAN(LCL_2) MEAN(UCL_2) BY TIME /MISSING=LISTWISE . *Return plots. COMPUTE TS_1_lag1 = LAG(TS_1) . COMPUTE TS_2_lag1 = LAG(TS_2) . COMPUTE TS_3_lag1 = LAG(TS_3) . EXECUTE . IGRAPH /VIEWNAME=&#39;Scatterplot&#39; /X1 = VAR(TS_1_lag1) TYPE = SCALE /Y = VAR(TS_1) TYPE = SCALE /COORDINATE = VERTICAL /X1LENGTH=3.0 /YLENGTH=3.0 /X2LENGTH=3.0 /CHARTLOOK=&#39;NONE&#39; /SCATTER COINCIDENT = NONE. EXE. IGRAPH /VIEWNAME=&#39;Scatterplot&#39; /X1 = VAR(TS_2_lag1) TYPE = SCALE /Y = VAR(TS_2) TYPE = SCALE /COORDINATE = VERTICAL /X1LENGTH=3.0 /YLENGTH=3.0 /X2LENGTH=3.0 /CHARTLOOK=&#39;NONE&#39; /SCATTER COINCIDENT = NONE. EXE. IGRAPH /VIEWNAME=&#39;Scatterplot&#39; /X1 = VAR(TS_3_lag1) TYPE = SCALE /Y = VAR(TS_3) TYPE = SCALE /COORDINATE = VERTICAL /X1LENGTH=3.0 /YLENGTH=3.0 /X2LENGTH=3.0 /CHARTLOOK=&#39;NONE&#39; /SCATTER COINCIDENT = NONE. EXE. Were you surprised finding out Timeseries 3 is the logisitc map in the chaotic regime? It ruly ‘looks’ random (according to PACF). "],
["using-r-to-fit-the-solutions.html", "D.3 Using R to fit the solutions", " D.3 Using R to fit the solutions There are several packages that can perform nonlinear regression analysis, the function most resembling the approach used by SPSS is nls in the default stats package. The easiest way to do this is to first define your function (i.e., the solution) and then fit it using starting values for the parameters. library(rio) df &lt;- import(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/BasicTSA_nonlinreg/GrowthRegression.sav&quot;, setclass = &quot;tbl_df&quot;) # Logistic growth # Same as SPSS syntax: PRED_=K*Yzero/(Yzero + (K-Yzero) * EXP(-1*(r * K * Time))). log.eq &lt;- function(Yzero, r, K, Time) { K*Yzero/(Yzero + (K-Yzero) * exp(-1*(r * K * Time))) } There is one drawback and you can read about in the help pages: Warning Do not use nls on artificial “zero-residual” data. This means, “do not use it on data generated by a deterministic model which has no residual error”, which is exactly what the timeseries in this assignment is, it is the output of the quadratic map in the chaotic regime. So, this will give an error: # Fit this function ... gives an error # The list after &#39;start&#39; provides the initial values m.log &lt;- nls(Yt ~ log.eq(Yzero, r, K, Time), data = df, start = list(Yzero=.01, r=.01, K=1), trace = T) It is possible to fit these ideal data using package minpack.lm, which contains function nlsM. library(minpack.lm) m.log &lt;- nlsLM(Yt ~ log.eq(Yzero, r, K, Time), data = df, start = list(Yzero = .01, r=.01, K=0.1)) summary(m.log) Formula: Yt ~ log.eq(Yzero, r, K, Time) Parameters: Estimate Std. Error t value Pr(&gt;|t|) Yzero 7.055e-03 8.983e-05 78.53 &lt;2e-16 *** r 1.491e-01 4.170e-04 357.59 &lt;2e-16 *** K 1.002e+00 4.376e-04 2289.42 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.002865 on 97 degrees of freedom Number of iterations to convergence: 13 Achieved convergence tolerance: 1.49e-08 In order to look at the model prediction, we use predict() which is defined for almost all modelfitting functions in R Ypred &lt;- predict(m.log) plot(ts(df$Yt), col=&quot;gray40&quot;, lwd=5, ylab = (&quot;Yt | Ypred&quot;)) lines(Ypred, col=&quot;gray80&quot;, lwd=2, lty=2) Then we do a polynomial regression using lm: # Mimic the SPSS syntax attach(df) df$T1 &lt;- Yt * Time df$T2 &lt;- Yt * (Time^2) df$T3 &lt;- Yt * (Time^3) df$T4 &lt;- Yt * (Time^4) detach(df) m.poly &lt;- lm(Yt ~ T1 + T2 + T3 + T4, data=df) summary(m.poly) Call: lm(formula = Yt ~ T1 + T2 + T3 + T4, data = df) Residuals: Min 1Q Median 3Q Max -0.0117491 -0.0046800 -0.0000683 0.0045719 0.0112732 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.113e-02 1.258e-03 16.80 &lt;2e-16 *** T1 6.366e-02 7.169e-04 88.80 &lt;2e-16 *** T2 -1.497e-03 3.100e-05 -48.28 &lt;2e-16 *** T3 1.510e-05 4.425e-07 34.12 &lt;2e-16 *** T4 -5.529e-08 2.055e-09 -26.90 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.005506 on 95 degrees of freedom Multiple R-squared: 0.9998, Adjusted R-squared: 0.9998 F-statistic: 1.264e+05 on 4 and 95 DF, p-value: &lt; 2.2e-16 Then, predict and plot! Ypoly &lt;- predict(m.poly) plot(ts(Ypoly), col=&quot;blue1&quot;, lwd=2, ylab = (&quot;Ypoly (blue) | Ypred (red)&quot;)) lines(Ypred, col=&quot;red1&quot;, lwd=2) SPSS computes an \\(r^2\\) value for nonlinear regression models, which doesn’t make a lot of sense if you think about it. Here we van just compare the residual errors: Polynomial regression: \\(0.005506\\) Analytic solution: \\(0.002865\\)&quot; Slightly less residual error for the analytic solution, using less parameters to fit the model (3 vs. 5). More important:, the paramters of the analytic solution have a direct interpretation in terms of growth processes. -->"],
["phase-space-reconstruction-and-recurrence-quantification-analysis-rqa.html", "E Phase Space Reconstruction and Recurrence Quantification Analysis (RQA)", " E Phase Space Reconstruction and Recurrence Quantification Analysis (RQA) First load some libraries . library(devtools) library(plyr) library(dplyr) library(fractal) library(plot3D) library(crqa) library(dygraphs) library(lattice) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) "],
["PSRsol.html", "E.1 Phase Space Reconstruction of the Lorenz Attractor", " E.1 Phase Space Reconstruction of the Lorenz Attractor | Jump to assignment | Instead of rgl::plot3d we use plot3D::scatter3D to display the Lorenz attractor, because the 3D interactive plot doesn’t run inside this webbook. N &lt;- 3000 scatter3D(lorenz[1:N, 1], lorenz[1:N,2], lorenz[1:N,3], pch=&quot;.&quot;, col = &quot;black&quot;, colkey = FALSE, type = &quot;l&quot;) The three coupled equations of the Lorenz system constitute the three dimensions (X,Y and Z) of the state space of the system. lxyz &lt;- data.frame(t=1:N, lorenz[1:N, ]) dygraph(lxyz, main = &quot;Lorenz System - Chaotic Regime&quot;) %&gt;% dyHighlight(highlightCircleSize = 5, hideOnMouseOut = FALSE) %&gt;% dyOptions(colors = RColorBrewer::brewer.pal(3, &quot;Set2&quot;), drawGrid = FALSE) %&gt;% dyLegend(showZeroValues = TRUE) Embedding Lag Determine the embedding lag for optimal reconstruction. Use average mutual information as the criterion for selecting a delay. lagX &lt;- timeLag(lxyz$X, method = &quot;mutual&quot;, plot.data = TRUE) False Nearest Neighbours Determine how many points in the state space turn out to be neigbours if embedding dimension \\(n\\) is added to the state space \\(X(t), X(t + \\tau), \\ldots, X(t + n*\\tau)\\). (fnnX &lt;- FNN(lxyz$X, tlag = lagX)) False Nearest Neighbors for lxyz$X ---------------------------------- Series points : 3000 Embedding dimension(s) : 1 2 3 4 5 Time lag : 17 Oribital lag : 1 Neighbor tolerance (rtol) : 10 Attractor tolerance (atol) : 2 Test results (%): E=1 E=2 E=3 E=4 E=5 rtol 97.84 5.97 0 0 0 atol 1.42 0.00 0 0 0 combined 97.84 5.97 0 0 0 plotRP.fnn(fnnX) If you use plotRP.fnn(), the figure shows the combined result (grey circles), the maximum of two methods for determining whether points are false neighbours (atol and rtol). If you want to know the details, see the manual entry for FNN(). The combined method will give you the best value for embedding dimension in most contexts. Except… when you try to model a low dimensional system like the Predator-Prey system. Plot the reconstructed phase space Objects of the class embedSeries generated by embedSeries() have a special plot function: (lxxx &lt;- embedSeries(lxyz$X,dimension = 3, tlag = lagX)) Embedding matrix for lxyz$X --------------------------- Number of points : 2966 Embedding dimension(s) : 1 2 3 Time lag : 17 plot(lxxx, pch=&quot;.&quot;, col = &quot;black&quot;, type = &quot;l&quot;) Or use plot3D plot3D::scatter3D(lxxx[, 1], lxxx[,2], lxxx[,3], pch=&quot;.&quot;, col = &quot;black&quot;, colkey = FALSE, type = &quot;l&quot;) "],
["phase-space-reconstruction-of-the-predator-prey-system.html", "E.2 Phase Space Reconstruction of the Predator-Prey system", " E.2 Phase Space Reconstruction of the Predator-Prey system Get some iterations from the Predator-Prey system. # Parameters N &lt;- 2048 a &lt;- d &lt;- 1 b &lt;- c &lt;- 2 R0 &lt;- F0 &lt;- 0.1 R &lt;- as.numeric(c(R0, rep(NA,N-1))) F &lt;- as.numeric(c(F0, rep(NA,N-1))) # Time constant delta &lt;- 0.01 # Numerical integration of the logistic differential equation l_ply(seq_along(R), function(t){ R[[t+1]] &lt;&lt;- (a - b * F[t]) * R[t] * delta + R[t] F[[t+1]] &lt;&lt;- (c * R[t] - d) * F[t] * delta + F[t] }) Let’s plot the results. dygraph(data.frame(time=1:2049,Rabbits=R,Foxes=F), main = &quot;Predator-Prey System&quot;) %&gt;% dyHighlight(highlightCircleSize = 5, hideOnMouseOut = FALSE) %&gt;% dyOptions(colors = RColorBrewer::brewer.pal(3, &quot;Set2&quot;), drawGrid = FALSE) %&gt;% dyLegend(showZeroValues = TRUE) xyplot(R ~ F, pch = 16) Embedding Lag Determine the embedding lag for optimal reconstruction. Use average mutual information of Rabbits or Foxes as the criterion for selecting a delay. lagR &lt;- timeLag(R, method = &quot;mutual&quot;, plot.data = TRUE) False Nearest Neighbours Determine how many points in the state space turn out to be neigbours if embedding dimension \\(n\\) is added to the state space \\(R(t), R(t + \\tau), \\ldots, R(t + n*\\tau)\\). (fnnR &lt;- FNN(R, tlag = lagR)) False Nearest Neighbors for R ----------------------------- Series points : 2049 Embedding dimension(s) : 1 2 3 4 5 Time lag : 334 Oribital lag : 1 Neighbor tolerance (rtol) : 10 Attractor tolerance (atol) : 2 Test results (%): E=1 E=2 E=3 E=4 E=5 rtol 86.3 29.5 9.76 0 0 atol 10.0 0.0 0.00 0 0 combined 86.3 29.5 9.76 0 0 plotRP.fnn(fnnR) Plot the reconstructed phase space Objects of the class embedSeries generated by embedSeries() have a special plot function: (lRR &lt;- embedSeries(R,dimension = 2, tlag = lagR)) Embedding matrix for R ---------------------- Number of points : 1715 Embedding dimension(s) : 1 2 Time lag : 334 plot(lRR, pch=&quot;.&quot;, col = &quot;black&quot;, type = &quot;l&quot;) -->"],
["recurrence-quantification-analysis.html", "F Recurrence Quantification Analysis ", " F Recurrence Quantification Analysis "],
["RQAsol.html", "F.1 RQA of the Lorenz system", " F.1 RQA of the Lorenz system | Jump to assignment | Perform an RQA on the reconstructed state space of the Lorenz system. You’ll need a radius (also called: threshold) in order to decide which points are close together (recurrent). crqa provides a function which will automatically select the best parametersettings: optimizeParam() Best way to ensure you are using the same parameters in each function is to create some lists with parametersettings (check the crqa manual to figure out what these parameters mean): # General settings for `crqa()` par0 &lt;- list(rescale = 1, normalize = 0, mindiagline = 2, minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, side = &quot;lower&quot;, checkl = list(do = FALSE, thrshd = 3, datatype = &quot;categorical&quot;,pad = TRUE) ) # Settings for `optimizeParam()` par &lt;- list(lgM = 20, steps = seq(1, 6, 1), radiusspan = 100, radiussample = 40, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, fnnpercent = 10, typeami = &quot;mindip&quot;) Get the optimal parameters using a radius which will give us 2%-5% recurrent points. (ans &lt;- optimizeParam(ts1 = lxyz$X, ts2 = lxyz$X, par = par, min.rec = 2, max.rec = 5)) $radius [1] 17.2439 $emddim [1] 2 $delay [1] 18 Run the RQA analysis using the same settings with which the parameters were found. crqaOutput &lt;- crqa(ts1 = lxyz$X, ts2 = lxyz$X, delay = ans$delay, embed = ans$emddim, radius = ans$radius, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, side = par0$side, checkl = par0$checkl ) The output of crqa is a list with recurrence measures, the last entry is the recurrence plot. It is represented as a so-called sparse-matrix. This representation severely decreases the amount of memory occupied by the recurrence matrix. It is basically a list of indices of cells that contain a \\(1\\). The \\(0\\) do not need to be stored. In order to plot this matrix you could use image(), but this does not produce the recurrence plot as they are usually displayed, the y-axis needs to be flipped. We created a function which will take as input the list output of crqa, together with lattice::levelplot the recurrence matrix can be created. If you have loaded (or sourced) the nlRtsa package you can call plotRP.crqa(crqaOutput). plotRP.crqa(crqaOutput) "],
["rqa-of-the-circle-tracing-task.html", "F.2 RQA of the Circle Tracing Task", " F.2 RQA of the Circle Tracing Task Get the data generated during the lecture. library(rio) xy &lt;- import(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/RQA_circletrace/mouse_circle_xy.csv&quot;) Embedding lag Determine the embedding lag. (lagXc &lt;- timeLag(xy$x, method = &quot;mutual&quot;, plot.data = TRUE)) [1] 86 attr(,&quot;data&quot;) [,1] [1,] 1.8748360 [2,] 1.8320247 [3,] 1.7597725 [4,] 1.6660259 [5,] 1.5695448 [6,] 1.4851768 [7,] 1.4155511 [8,] 1.3580141 [9,] 1.3099462 [10,] 1.2697228 [11,] 1.2346290 [12,] 1.2030061 [13,] 1.1738016 [14,] 1.1461442 [15,] 1.1202508 [16,] 1.0955800 [17,] 1.0723971 [18,] 1.0504011 [19,] 1.0295328 [20,] 1.0092911 [21,] 0.9901784 [22,] 0.9716143 [23,] 0.9539218 [24,] 0.9366875 [25,] 0.9201264 [26,] 0.9038439 [27,] 0.8881944 [28,] 0.8729000 [29,] 0.8582478 [30,] 0.8440019 [31,] 0.8302720 [32,] 0.8170165 [33,] 0.8042412 [34,] 0.7918786 [35,] 0.7798863 [36,] 0.7682832 [37,] 0.7568578 [38,] 0.7457565 [39,] 0.7349257 [40,] 0.7244155 [41,] 0.7141488 [42,] 0.7039729 [43,] 0.6938999 [44,] 0.6839407 [45,] 0.6741110 [46,] 0.6644834 [47,] 0.6549952 [48,] 0.6455323 [49,] 0.6361238 [50,] 0.6269657 [51,] 0.6179179 [52,] 0.6093046 [53,] 0.6010238 [54,] 0.5932492 [55,] 0.5856680 [56,] 0.5783174 [57,] 0.5710348 [58,] 0.5639222 [59,] 0.5569276 [60,] 0.5501662 [61,] 0.5434106 [62,] 0.5368340 [63,] 0.5304724 [64,] 0.5241171 [65,] 0.5178612 [66,] 0.5117279 [67,] 0.5056025 [68,] 0.4997365 [69,] 0.4941006 [70,] 0.4886011 [71,] 0.4832675 [72,] 0.4780774 [73,] 0.4732322 [74,] 0.4685945 [75,] 0.4640545 [76,] 0.4596915 [77,] 0.4553726 [78,] 0.4510386 [79,] 0.4469948 [80,] 0.4451781 [81,] 0.4438201 [82,] 0.4427879 [83,] 0.4411173 [84,] 0.4379825 [85,] 0.4335339 [86,] 0.4279071 [87,] 0.4235801 [88,] 0.4247762 [89,] 0.4280886 [90,] 0.4296760 [91,] 0.4286545 [92,] 0.4248078 [93,] 0.4197830 [94,] 0.4166983 [95,] 0.4162246 [96,] 0.4192973 [97,] 0.4245453 [98,] 0.4313695 [99,] 0.4381306 [100,] 0.4425022 [101,] 0.4452126 [102,] 0.4481473 [103,] 0.4521713 [104,] 0.4546178 [105,] 0.4562708 [106,] 0.4612016 [107,] 0.4687082 [108,] 0.4766385 [109,] 0.4846580 [110,] 0.4903264 [111,] 0.4931311 [112,] 0.4937906 [113,] 0.4931051 [114,] 0.4901800 [115,] 0.4862715 [116,] 0.4832530 [117,] 0.4829012 [118,] 0.4874575 [119,] 0.4968100 [120,] 0.5082572 [121,] 0.5202744 [122,] 0.5334663 [123,] 0.5470788 [124,] 0.5590956 [125,] 0.5690183 [126,] 0.5760787 [127,] 0.5810159 [128,] 0.5845512 [129,] 0.5854967 [130,] 0.5855992 [131,] 0.5874306 [132,] 0.5913599 [133,] 0.5955356 [134,] 0.5987059 [135,] 0.6024263 [136,] 0.6063072 [137,] 0.6093108 [138,] 0.6111368 [139,] 0.6118817 [140,] 0.6116046 [141,] 0.6124853 [142,] 0.6154613 [143,] 0.6188560 [144,] 0.6211657 [145,] 0.6219710 [146,] 0.6197909 [147,] 0.6144581 [148,] 0.6066394 [149,] 0.5968153 [150,] 0.5857086 [151,] 0.5734599 [152,] 0.5592622 [153,] 0.5423187 [154,] 0.5234938 [155,] 0.5033835 [156,] 0.4824005 [157,] 0.4633789 [158,] 0.4461670 [159,] 0.4296615 [160,] 0.4136314 [161,] 0.3978943 [162,] 0.3822670 [163,] 0.3669812 [164,] 0.3524403 [165,] 0.3386489 [166,] 0.3257262 [167,] 0.3149718 [168,] 0.3062398 [169,] 0.2990444 [170,] 0.2947886 [171,] 0.2939093 [172,] 0.2937713 [173,] 0.2929135 [174,] 0.2903755 [175,] 0.2865028 [176,] 0.2811382 [177,] 0.2744351 [178,] 0.2677695 [179,] 0.2617230 [180,] 0.2559572 [181,] 0.2506732 [182,] 0.2451949 [183,] 0.2392856 [184,] 0.2330739 [185,] 0.2262502 [186,] 0.2194679 [187,] 0.2136034 [188,] 0.2098218 [189,] 0.2079393 [190,] 0.2069279 [191,] 0.2062112 [192,] 0.2050514 [193,] 0.2027659 [194,] 0.1989667 [195,] 0.1940491 [196,] 0.1887533 [197,] 0.1837352 [198,] 0.1802085 [199,] 0.1787252 [200,] 0.1792536 [201,] 0.1802651 [202,] 0.1810710 attr(,&quot;lags&quot;) [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 [103] 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 [120] 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 [137] 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 [154] 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 [171] 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 [188] 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 attr(,&quot;method&quot;) [1] &quot;mutual&quot; False Nearest Neighbours Run FNN to figure out the number of dimensions … we should get at least 2 (x and y, but maybe some other factors influenced the dynamics). In this case (as is the case with all real world data) we have to rescale the timeseries. A z-score transform (normalise) is usually ok, but sometimes a unit-scale transform (\\(0 \\cdots 1\\) may be necessary). (fnnXc &lt;- FNN(scale(xy$x), tlag = lagXc)) False Nearest Neighbors for scale(xy$x) --------------------------------------- Series points : 3500 Embedding dimension(s) : 1 2 3 4 5 Time lag : 86 Oribital lag : 1 Neighbor tolerance (rtol) : 10 Attractor tolerance (atol) : 2 Test results (%): E=1 E=2 E=3 E=4 E=5 rtol 99.3 37.42 4.77 0.381 0.0326 atol 19.1 0.55 0.00 0.000 0.0000 combined 99.3 37.42 4.77 0.381 0.0326 plotRP.fnn(fnnXc) We got a lag of 86 and an embedding dimension of \\(3\\) or \\(4\\). Three is actually not so strange, although we have recorded just 2 coordinates, there’s acceleration differences due to the biomechanics of the arm and hand, or resistance of the mouse on the surface, etc. Let’s see what package crqa tells us to choose automagically. We got a lag/delay of \\(86\\), let’s set the maximum lags to consider to \\(100\\) (lgM). # General settings for `crqa()` par0 &lt;- list(rescale = 1, normalize = 0, mindiagline = 2, minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, side = &quot;lower&quot;, checkl = list(do = FALSE, thrshd = 3, pad = TRUE) ) # Settings for `optimizeParam()` par &lt;- list(lgM = 100, steps = seq(1, 6, 1), radiusspan = 100, radiussample = 40, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, fnnpercent = 10, typeami = &quot;mindip&quot;) (ansCircle &lt;- optimizeParam(ts1 = xy$x, ts2 = xy$x, par = par, min.rec = 2, max.rec = 5)) $radius [1] 17.96495 $emddim [1] 3 $delay [1] 94 The suggested embedding dimension is \\(3\\), the delay (lag) is somewhat higher, \\(94\\) than 86. These different values for the embedding lag shouldn’t influence the global pattern of the resuts. Run the RQA Run RQA with the optimal parameters. crqaOutput &lt;- crqa(ts1 = xy$x, ts2 = xy$x, delay = ansCircle$delay, embed = ansCircle$emddim, radius = ansCircle$radius, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, side = par0$side, checkl = par0$checkl ) plotRP.crqa(crqaOutput) Run RQA with embedding lag 86 to check if the resulta will be very different. crqaOutput2 &lt;- crqa(ts1 = xy$x, ts2 = xy$x, delay = lagXc, embed = ansCircle$emddim, radius = ansCircle$radius, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, side = par0$side, checkl = par0$checkl ) plotRP.crqa(crqaOutput2) {#RQA} "],
["CRQAsol.html", "F.3 Assignment: Cross Recurrence Quantification Analysis", " F.3 Assignment: Cross Recurrence Quantification Analysis | Jump to assignment | Create two variables for CRQA analysis: y1 &lt;- sin(1:900*2*pi/67) y2 &lt;- sin(.01*(1:900*2*pi/67)^2) You have just created two sine waves. We’ll examine if and how they are coupled in a shared phase space. As a first step plot them. Find an embedding delay (using mutual information) and an embedding dimension (if you calculate an embedding dimension using package fractal for each signal seperately, as a rule of thumb use the highest embedding dimension you find in further analyses). # General settings for `crqa()` par0 &lt;- list(rescale = 0, normalize = 0, mindiagline = 2, minvertline = 2, tw = 0, whiteline = FALSE, recpt = FALSE, side = &quot;both&quot;, checkl = list(do = FALSE, thrshd = 3, datatype = &quot;categorical&quot;,pad = TRUE) ) # Settings for `optimizeParam()` par &lt;- list(lgM = 20, steps = seq(1, 6, 1), radiusspan = 100, radiussample = 40, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, fnnpercent = 10, typeami = &quot;mindip&quot;) We can now create a cross recurrence matrix. Fill in the values you decided on. You can choose a radius automatically, look in the crqa manual. Note: There is no rescaling of data, the sines were created in the same range. You can plot a matrix using image(). You could also check package nonlinearTseries. If you sourced the nlRtsa library, use plotRP.crqa() Get the optimal parameters using a radius which will give us 2%-5% recurrent points. (ans &lt;- optimizeParam(ts1 = y1, ts2 = y2, par = par, min.rec = 2, max.rec = 5)) $radius [1] 0.2128611 $emddim [1] 2 $delay [1] 15 Run the CRQA. crqaOutput &lt;- crqa(ts1 = y1, ts2 = y2, delay = ans$delay, embed = ans$emddim, radius = ans$radius, normalize = par0$normalize, rescale = par0$rescale, mindiagline = par0$mindiagline, minvertline = par0$minvertline, tw = par0$tw, whiteline = par0$whiteline, recpt = par0$recpt, side = par0$side, checkl = par0$checkl ) Can you understand what is going on? Explain the the lack of recurrent points at the beginning of the time series. plotRP.crqa(crqaOutput) Examine the synchronisation under the diagonal LOS. Look in the manual of crqa or Coco &amp; Dale (2014). Make a plot of the diagonal profie. How far is the peak in RR removes from 0 (Line of Synchronisation)? This is based on the timeseries (without embedding) win &lt;- 50 (res &lt;- drpdfromts(y1, y2, ws = win, datatype = &#39;continuous&#39;, radius = ans$radius))[2:3] $maxrec [1] 0.3080357 $maxlag [1] 55 plot(-win:win,res$profile,type = &quot;l&quot;, lwd = 5, xlab = &quot;Delays&quot;, ylab = &quot;Recurrence&quot;) abline(v=0) To get the diagonal profile from the recurrence plot, use spdiags(). dprofile &lt;- spdiags(crqaOutput$RP) plot(-win:win,colMeans(dprofile$B[, between(dprofile$d, -win,win)]), type=&quot;l&quot;, lwd = 5, xlab = &quot;Delays&quot;, ylab = &quot;Recurrence&quot;) abline(v=0) Perform the same steps with a shuffled version (or use surrogate analysis!) of the data of timeseries \\(y1\\). You can use the embedding parameters you found earlier. "],
["catCRQAsol.html", "F.4 Assignment: Categorical CRQA", " F.4 Assignment: Categorical CRQA The solutions are basically in the paper by Coco &amp; Dale, the purpose here is to introduce the parametersettings used for CRQA. | Jump to assignment | -->"],
["netssol.html", "G Complex Networks", " G Complex Networks | Jump to assignment | To complete these assignments you need: library(igraph) library(qgraph) library(devtools) source_url(&quot;https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R&quot;) Great resources for learning about the different measures: Strogatz, S. H. (2001). Exploring complex networks. Nature, 410(6825), 268-276. Bullmore, E., &amp; Sporns, O. (2009). Complex brain networks: graph theoretical analysis of structural and functional systems. Nature reviews. Neuroscience, 10(3), 186-98. doi:10.1038/nrn2575 "],
["basic-graphs-1.html", "G.1 Basic graphs", " G.1 Basic graphs # Create a small graph and plot it g &lt;- graph.ring(20) plot(g) # Get the degree of each node degree(g) [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 # Get the average path length average.path.length(g) [1] 5.263158 # Create a &quot;Small world&quot; graph g &lt;- watts.strogatz.game(1, 20, 5, 0.05) # Get the degree of each node degree(g) [1] 10 10 9 11 10 8 9 10 10 10 9 10 10 10 12 11 11 10 10 10 # Get the average path length average.path.length(g) [1] 1.473684 # Get the transitivity transitivity(g) [1] 0.6185226 plot(g) # Directed &quot;scale free&quot; graph set.seed(456) g &lt;- barabasi.game(20) plot(g) # Get the degree of each node degree(g) [1] 7 4 2 4 1 2 1 3 1 2 1 1 2 1 1 1 1 1 1 1 # Get the average path length average.path.length(g) [1] 1.571429 # Get the transitivity transitivity(g) [1] 0 To check for a power-law, remember what the scaling relation is supposed to represent: A relation between number of nodes of a particular size, with the size of those nodes. The following produces an unsorted graph: plot(log(1:20),log(degree(g))) So, we need to sort: plot(log(1:20),sort(log(degree(g)), decreasing = TRUE)) Or even better, sort and bin it: d &lt;- degree(g) y &lt;- hist(d,breaks=0.5:(max(d)+0.5),plot=TRUE, xlab = &quot;Node degree&quot;)$counts op &lt;-par(&quot;xlog&quot;,&quot;ylog&quot;) plot(1:length(y),rev(y), xlim = c(length(y),1), xlab = &quot;Node degree (log)&quot;, ylab = &quot;Frequency of Node degree (log)&quot;, log= c(&quot;xy&quot;)) par(op) (alpha=coef(lm(rev(log1p(y)) ~ log1p(1:length(y))))) (Intercept) log1p(1:length(y)) -0.992961 1.283407 "],
["social-networks-1.html", "G.2 Social Networks", " G.2 Social Networks Social network of friendships between 34 members of a karate club at a US university in the 1970s. See W. W. Zachary, An information flow model for conflict and fission in small groups, Journal of Anthropological Research 33, 452-473 (1977). # Community membership karate &lt;- graph.famous(&quot;Zachary&quot;) wc &lt;- walktrap.community(karate) plot(wc, karate) modularity(wc) [1] 0.3532216 membership(wc) [1] 1 1 2 1 5 5 5 1 2 2 5 1 1 2 3 3 5 1 3 1 3 1 3 4 4 4 3 4 2 3 2 2 3 3 # What does this matrix look like? get.adjacency(karate) 34 x 34 sparse Matrix of class &quot;dgCMatrix&quot; [1,] . 1 1 1 1 1 1 1 1 . 1 1 1 1 . . . 1 . 1 . 1 . . . . . . . . . 1 . . [2,] 1 . 1 1 . . . 1 . . . . . 1 . . . 1 . 1 . 1 . . . . . . . . 1 . . . [3,] 1 1 . 1 . . . 1 1 1 . . . 1 . . . . . . . . . . . . . 1 1 . . . 1 . [4,] 1 1 1 . . . . 1 . . . . 1 1 . . . . . . . . . . . . . . . . . . . . [5,] 1 . . . . . 1 . . . 1 . . . . . . . . . . . . . . . . . . . . . . . [6,] 1 . . . . . 1 . . . 1 . . . . . 1 . . . . . . . . . . . . . . . . . [7,] 1 . . . 1 1 . . . . . . . . . . 1 . . . . . . . . . . . . . . . . . [8,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [9,] 1 . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 1 [10,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 [11,] 1 . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . [12,] 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [13,] 1 . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [14,] 1 1 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 [15,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 [16,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 [17,] . . . . . 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . [18,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [19,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 [20,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 [21,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 [22,] 1 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . [23,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 [24,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . 1 . . 1 1 [25,] . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 . . . 1 . . [26,] . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . 1 . . [27,] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . . 1 [28,] . . 1 . . . . . . . . . . . . . . . . . . . . 1 1 . . . . . . . . 1 [29,] . . 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . 1 [30,] . . . . . . . . . . . . . . . . . . . . . . . 1 . . 1 . . . . . 1 1 [31,] . 1 . . . . . . 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1 [32,] 1 . . . . . . . . . . . . . . . . . . . . . . . 1 1 . . 1 . . . 1 1 [33,] . . 1 . . . . . 1 . . . . . 1 1 . . 1 . 1 . 1 1 . . . . . 1 1 1 . 1 [34,] . . . . . . . . 1 1 . . . 1 1 1 . . 1 1 1 . 1 1 . . 1 1 1 1 1 1 1 . 34 people sre in the karate class, a 1 is printed if they know each other. You can do a clustering analysis for any 0-1 matrix! "],
["small-world-index-and-degree-distribution-1.html", "G.3 Small World Index and Degree Distribution", " G.3 Small World Index and Degree Distribution Select and run all the code below This will compute the Small World Index and compute the Power Law slope Fit of Small world networks and Scale-free networks Compare the measures! # Initialize set.seed(660) layout1=layout.kamada.kawai k=3 n=50 # Setup plots par(mfrow=c(2,3)) # Strogatz rewiring probability = .00001 p &lt;- 0.00001 p1 &lt;- plotSW(n=n,k=k,p=p) PLF1&lt;- PLFsmall(p1) p11 &lt;- plot(p1,main=&quot;p = 0.00001&quot;, layout = layout1, xlab=paste(&quot;FON = &quot;,round(mean(neighborhood.size(p1,order=1)), digits=1), &quot;\\nSWI = &quot;, round(SWtestE(p1,N=100)$valuesAV$SWI,digits=2), &quot;\\nPLF = NA&quot;, sep=&quot;&quot;)) # Strogatz rewiring probability = .01 p &lt;- 0.01 p2 &lt;- plotSW(n=n,k=k,p=p) PLF2&lt;- PLFsmall(p2) p22 &lt;- plot(p2,main=&quot;p = 0.01&quot;, layout = layout1, xlab = paste(&quot;FON = &quot;,round(mean(neighborhood.size(p2,order=1)),digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p2,N=100)$valuesAV$SWI, digits=2),&quot;\\nPLF = &quot;,round(PLF2,digits=2),sep=&quot;&quot;)) # Strogatz rewiring probability = 1 p &lt;- 1 p3 &lt;- plotSW(n=n,k=k,p=p) PLF3&lt;- PLFsmall(p3) p33 &lt;- plot(p3,main=&quot;p = 1&quot;, layout = layout1, xlab = paste(&quot;FON = &quot;, round(mean(neighborhood.size(p3,order=1)), digits=1), &quot;\\nSWI = &quot;, round(SWtestE(p3,N=100)$valuesAV$SWI, digits=2), &quot;\\nPLF = &quot;, round(PLF3,digits=2), sep=&quot;&quot;)) set.seed(200) # Barabasi power = 0 p4 &lt;- plotBA(n=n,pwr=0,out.dist=hist(degree(p1,mode=&quot;all&quot;),breaks=(0:n),plot=FALSE)$density) PLF4&lt;- PLFsmall(p4) p44 &lt;- plot(p4,main=&quot;power = 0&quot;,layout = layout1, xlab = paste(&quot;FON = &quot;, round(mean(neighborhood.size(p4, order=1)), digits=1),&quot;\\nSWI = &quot;, round(SWtestE(p4, N=100)$valuesAV$SWI, digits=2), &quot;\\nPLF = &quot;, round(PLF4, digits=2),sep=&quot;&quot;)) # Barabasi power = 2 p5 &lt;- plotBA(n=n,pwr=2,out.dist = hist(degree(p2,mode=&quot;all&quot;),breaks=(0:n),plot=FALSE)$density) PLF5&lt;- PLFsmall(p5) p55 &lt;- plot(p5,main=&quot;power = 2&quot;, layout = layout1, xlab=paste(&quot;FON = &quot;, round(mean(neighborhood.size(p5, order=1)), digits=1), &quot;\\nSWI = &quot;, round(SWtestE(p5, N=100)$valuesAV$SWI, digits=2), &quot;\\nPLF = &quot;, round(PLF5, digits=2), sep=&quot;&quot;)) # Barabasi power = 4 p6 &lt;- plotBA(n=n,pwr=4, out.dist = hist(degree(p3,mode=&quot;all&quot;), breaks=(0:n),plot=FALSE)$density) PLF6&lt;- PLFsmall(p6) p66 &lt;- plot(p6,main=&quot;power = 4&quot;,layout = layout1, xlab = paste(&quot;FON = &quot;, round(mean(neighborhood.size(p6, order=1)), digits=1), &quot;\\nSWI = &quot;, round(SWtestE(p6,N=100)$valuesAV$SWI, digits = 2),&quot;\\nPLF = &quot;,round(PLF6, digits=2), sep=&quot;&quot;)) par(mfrow=c(1,1)) The PLF fit is unreliable, there are not enough nodes and different values for degree. the SWI does seem to follow the small-wordledness / scale-free topology quite closely. -->"],
["bibliography.html", "Bibliography", " Bibliography "]
]
